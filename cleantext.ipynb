{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import string\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data = pd.read_csv('data/research_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdoc = list(data['full_text_x'][1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split('Acknowledgements')\n",
    "split('Abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<3x4483 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5567 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "vectorizor = TfidfVectorizer(stop_words = 'english')\n",
    "vectorizor.fit_transform(testdoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "frozenset({'seems', 'get', 'third', 'detail', 'beside', 'anyway', 'an', 'almost', 'further', 'even', 'keep', 'would', 'bill', 'fire', 'elsewhere', 'except', 'which', 'indeed', 'up', 'this', 'back', 'thick', 'five', 'through', 'bottom', 'still', 'myself', 'during', 'already', 'all', 'around', 'full', 'fifteen', 'beyond', 'at', 'since', 'give', 'fifty', 'into', 'on', 'toward', 'last', 'besides', 'they', 'thru', 'per', 'such', 'hence', 'call', 'my', 'latter', 'herself', 'nine', 'etc', 'under', 'not', 're', 'although', 'very', 'amount', 'often', 'whither', 'someone', 'nothing', 'but', 'hers', 'namely', 'more', 'herein', 'between', 'after', 'where', 'whoever', 'anyone', 'former', 'therein', 'therefore', 'others', 'co', 'several', 'will', 'thin', 'either', 'against', 'whom', 'must', 'else', 'those', 'nobody', 'noone', 'no', 'find', 'am', 'nor', 'thereupon', 'ours', 'whose', 'again', 'however', 'with', 'that', 'across', 'there', 'to', 'move', 'always', 'whether', 'otherwise', 'within', 'both', 'rather', 'whatever', 'over', 'anywhere', 'hereafter', 'himself', 'done', 'everything', 'much', 'yet', 'whence', 'whole', 'your', 'afterwards', 'has', 'con', 'thus', 'were', 'by', 'never', 'three', 'though', 'the', 'see', 'nevertheless', 'inc', 'she', 'whenever', 'less', 'four', 'mine', 'we', 'hereupon', 'empty', 'or', 'everyone', 'forty', 'now', 'became', 'first', 'out', 'other', 'wherever', 'being', 'take', 'until', 'and', 'be', 'together', 'same', 'un', 'seem', 'a', 'should', 'before', 'down', 'what', 'if', 'might', 'how', 'perhaps', 'most', 'below', 'alone', 'amongst', 'becomes', 'side', 'anyhow', 'us', 'cannot', 'do', 'its', 'neither', 'onto', 'him', 'in', 'was', 'only', 'once', 'de', 'describe', 'because', 'nowhere', 'ie', 'have', 'front', 'system', 'fill', 'ever', 'none', 'for', 'is', 'seeming', 'name', 'six', 'something', 'it', 'amoungst', 'eight', 'while', 'anything', 'some', 'whereupon', 'along', 'beforehand', 'sometimes', 'two', 'eg', 'one', 'couldnt', 'put', 'towards', 'cry', 'behind', 'ltd', 'twenty', 'itself', 'becoming', 'well', 'mill', 'thereafter', 'i', 'every', 'few', 'from', 'each', 'own', 'everywhere', 'among', 'somewhere', 'then', 'throughout', 'interest', 'whereby', 'whereas', 'hereby', 'why', 'latterly', 'without', 'her', 'his', 'here', 'eleven', 'may', 'wherein', 'via', 'found', 'show', 'hasnt', 'top', 'whereafter', 'twelve', 'another', 'made', 'when', 'please', 'cant', 'part', 'about', 'formerly', 'moreover', 'any', 'of', 'many', 'mostly', 'are', 'thereby', 'least', 'seemed', 'also', 'me', 'due', 'upon', 'had', 'could', 'serious', 'who', 'too', 'above', 'yourselves', 'been', 'as', 'their', 'you', 'thence', 'ourselves', 'become', 'go', 'yourself', 'sincere', 'so', 'sometime', 'hundred', 'than', 'ten', 'next', 'meanwhile', 'can', 'yours', 'them', 'off', 'our', 'sixty', 'these', 'he', 'somehow', 'enough', 'themselves'})\n"
     ]
    }
   ],
   "source": [
    "print(vectorizor.get_stop_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['00', '000', '0001', '0002', '00776v2', '01', '02', '02688', '02689', '03', '03827v1', '03980', '04', '04562', '05', '05742', '06', '060', '06069', '06270', '07', '08', '08023', '08142v2', '0850', '09', '10', '100', '1000', '10000', '1003', '101', '102121', '10451048', '10611064', '1069', '10k', '11', '11371155', '11601179', '117139', '11th', '12', '12011211', '1212', '1259', '13', '1301', '1308', '137154', '13831390', '14', '1404', '1409', '1410', '149198', '15', '1512', '15241534', '15321543', '15k', '16', '1603', '1604', '1605', '1606', '1608', '16k', '17', '1701', '1705', '17241734', '173', '173289', '1735', '17351780', '174181', '17461751', '18', '19', '192', '192374', '1926', '193k', '196201', '1993', '1997', '1998', '20', '200', '2000', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2011a', '2011b', '2012', '2013', '2013a', '2013b', '2014', '2015', '2016', '2016a', '2016b', '2017', '2018', '206902', '20k', '21', '2188', '21k', '22', '23', '230', '23k', '24', '243272', '24932537', '25', '255276', '26', '26732681', '27', '28', '280', '29', '29622970', '2x', '30', '300', '300dimensional', '30th', '31', '31043112', '31113119', '3115', '32', '33', '339373', '34', '343351', '35', '3570', '36', '37', '37763784', '3781', '38', '39', '3x', '40', '400', '404', '41', '42', '43', '437478', '44', '45', '46', '47', '48', '49', '4km', '50', '500', '5000', '50th', '51', '52', '52nd', '53', '5401', '55', '56', '567580', '57', '5701', '577585', '58', '59', '5k', '60', '61', '62', '63', '633', '64', '65', '66', '67', '68', '69', '6k', '6th', '70', '701708', '71', '72', '73', '74', '745752', '749', '75', '76', '77', '770778', '78k', '79', '795', '7th', '80', '81', '82', '8297', '83', '84', '85', '86', '87', '89', '90', '91', '92', '93', '94', '95', '95133', '96', '97', '97102', '98', '99', 'a1', 'aaai', 'aaron', 'ab', 'abhinav', 'ability', 'ability5', 'ablation', 'able', 'abs', 'abst', 'abstract', 'abstraction', 'abstractions', 'accelerated', 'acceptance', 'accepted', 'access', 'accordance', 'according', 'accordingly', 'account', 'accuracies', 'accuracy', 'accurate', 'achieve', 'achieves', 'achieving', 'acl', 'acoustic', 'acoustics', 'acpid', 'acpidump', 'act', 'actentrepresentation', 'action', 'actions', 'activate', 'activation', 'activities', 'activity', 'actor', 'acts', 'actual', 'actually', 'adadelta', 'adam', 'adapt', 'adaptation', 'adapted', 'adapter', 'adaptive', 'add', 'added', 'adding', 'addition', 'additional', 'address', 'addressing', 'adds', 'adj', 'admin', 'administrate', 'adobemit', 'adopted', 'adorable', 'advance', 'advances', 'advantage', 'advantageous', 'advice', 'aegis', 'afaik', 'affect', 'affected', 'affecting', 'affects', 'aggregate', 'ago', 'agree', 'agreement', 'ah', 'aha', 'ahn', 'ahold', 'aim', 'aimed', 'aims', 'air', 'aircrack', 'aistats', 'al', 'albeit', 'alex', 'algorithms', 'align', 'alignment', 'allocate', 'allocating', 'allow', 'allowed', 'allowing', 'alma', 'alonso', 'alphabet', 'alternative', 'amazed', 'amazing', 'american', 'ammar', 'amsn', 'ana', 'analysis', 'analyze', 'anastasopoulos', 'anders', 'andreas', 'andrew', 'ang', 'ann', 'annotated', 'annotations', 'annotator', 'announce', 'anns', 'annual', 'anonymized', 'answer', 'answering', 'antonios', 'anybody', 'anymore', 'apparently', 'appealing', 'appear', 'appears', 'append', 'appendix', 'appends', 'applet', 'application', 'applications', 'applied', 'apply', 'applying', 'appoint', 'approach', 'approaches', 'appropriate', 'appropriateness', 'approximate', 'approximately', 'approximation', 'apps', 'apt', 'aptitude', 'arcade', 'architecture', 'architectures', 'areas', 'aren', 'arent', 'arg', 'arg0', 'arg1', 'argue', 'argumentation', 'argyriou', 'arise', 'ark', 'arrange', 'array', 'arrows', 'art', 'article', 'artificial', 'arto', 'arxiv', 'aside', 'ask', 'asked', 'asking', 'aspell', 'asru', 'assets', 'assign', 'assigned', 'assist', 'assistance', 'associated', 'associating', 'association', 'assume', 'assumption', 'ata', 'atd', 'atp', 'attach', 'attempt', 'attempts', 'attention', 'atwell', 'atx', 'audio', 'audit', 'augenstein', 'augenstein4', 'augmented', 'aumix', 'aurlie', 'austin', 'auth', 'author', 'authors', 'automatic', 'automatically', 'auxiliary', 'available', 'avanade', 'average', 'averaged', 'averages', 'avg', 'aware', 'away', 'awesome', 'awfully', 'awk', 'b1', 'b2', 'ba', 'bach', 'bachouti', 'backpropagation', 'backup', 'backward', 'bad', 'bahdanau', 'bale', 'ballesteros', 'banchs', 'barahona', 'barbara', 'baron', 'bart', 'base', 'based', 'baseline', 'baselines', 'basename', 'bases', 'bashrc', 'basic', 'batches', 'bates', 'bathroom', 'baxter', 'bayes', 'bb', 'bc', 'beam', 'bear', 'beautiful', 'beep', 'begin', 'beginning', 'beginnings', 'begins', 'behavior', 'believe', 'belongs', 'belvin', 'ben', 'beneficial', 'beneficial7', 'benefit', 'bengio', 'benjamin', 'bernardino', 'best', 'better', 'bf', 'bg', 'bhagat', 'bi', 'bias', 'biasca', 'biases', 'bibletime', 'bidir', 'bidirectional', 'bieber', 'bif', 'bigger', 'bigram', 'bigrams', 'bilmes', 'bilmes2006', 'bilstm', 'bin', 'binary', 'bind', 'binfmt', 'bingel', 'bingel3', 'bio', 'biol', 'bios', 'bird', 'birthday', 'bit', 'bits', 'bittorrent', 'bj', 'blank', 'bleu', 'blob', 'block', 'blunsom', 'bn', 'bo', 'bold', 'bonnie', 'boot', 'bordes', 'bot', 'bottou', 'boulanger', 'bounds', 'bousmalis', 'boxes', 'brackets', 'bradbury', 'brainwaves', 'branding', 'break', 'brief', 'briefly', 'bring', 'broader', 'brockett', 'brody', 'broken', 'browse', 'bui', 'build', 'building', 'builds', 'builtin', 'burget', 'burn', 'buy', 'bzip2', 'c0', 'c1', 'ca', 'caglar', 'caiming', 'cal', 'called', 'cam', 'came', 'canada', 'candidate', 'capacity', 'capture', 'captures', 'card', 'careful', 'carl', 'carol', 'carried', 'carry', 'caruana', 'carvey', 'cascade', 'case', 'cases', 'cast', 'cat', 'categorized', 'category', 'cause', 'caused', 'causes', 'cca', 'cd', 'cell', 'center', 'centered', 'centre', 'cernocky', 'certain', 'certainly', 'cfdisk', 'challenge', 'challenging', 'chance', 'change', 'changed', 'changes', 'changing', 'channel', 'channels', 'chao', 'chapter', 'character', 'characteristics', 'characterized', 'characterizes', 'characters', 'charlin', 'chat', 'chatbots', 'check', 'chemin', 'chen', 'cherry', 'chgrp', 'chiang', 'ching', 'chmod', 'cho', 'choices', 'choose', 'chopra', 'chorowski', 'chown', 'chris', 'christian', 'christopher', 'chroot', 'chung', 'chunk', 'chunking', 'cikm', 'citeseer', 'cksum', 'cl', 'clark', 'class', 'classes', 'classification', 'classifies', 'classify', 'clean', 'clear', 'clearly', 'click', 'clipped', 'clipping', 'clock', 'close', 'closely', 'clothiaux', 'clustered', 'cmd', 'cmp', 'cmu', 'cnn', 'cnns', 'coarse', 'code', 'coding', 'cognitive', 'cohen', 'coherent', 'coheur', 'cohn', 'colleagues', 'collobert', 'collobert2011', 'colon', 'column', 'com', 'combination', 'combinations', 'combine', 'combined', 'combining', 'come', 'comes', 'comm', 'command', 'commands', 'comment', 'commercial', 'common', 'commonly', 'community', 'comparable', 'comparatively', 'compare', 'compared', 'compares', 'comparing', 'comparison', 'comparisons', 'compensate', 'competing', 'competitive', 'compile', 'compiz', 'complains', 'complete', 'completely', 'completeness', 'completion', 'complex', 'complexities', 'complimentary', 'component', 'components', 'composite', 'composition', 'compositionality', 'compress', 'comprises', 'computation', 'computational', 'compute', 'computed', 'computer', 'computes', 'computing', 'concatenated', 'concatenation', 'conceal', 'conditional', 'conditioned', 'conditions', 'conf', 'conference', 'confidence', 'confident', 'config', 'configurations', 'confirmed', 'confirming', 'conform', 'conjecture', 'conjugations', 'connect', 'connected', 'connections', 'consecutive', 'consider', 'considering', 'considers', 'consist', 'consistent', 'consistently', 'consisting', 'consists', 'consortium', 'constant', 'constituent', 'constrained', 'constraining', 'constraint', 'constraints', 'construct', 'constructing', 'constructive', 'contact', 'contain', 'contained', 'containing', 'contains', 'content', 'context', 'contexts', 'continue', 'continuous', 'contrast', 'contributed', 'control', 'controlled', 'controls', 'conv', 'conversation', 'conversational', 'conversations', 'conversely', 'convert', 'convex', 'convey', 'conveyed', 'convolution', 'convolutional', 'cook', 'copenhagen', 'copies', 'core', 'corpora', 'corpous', 'corpus', 'corr', 'corrado', 'correct', 'correlate', 'correlated', 'correlates', 'correlation', 'correspond', 'corresponding', 'corresponds', 'cos', 'cost', 'countdown', 'couple', 'courville', 'cp', 'cpu', 'crafted', 'crawl', 'create', 'created', 'crf', 'crfs', 'cristina', 'criteria', 'critical', 'cron', 'crontab', 'cross', 'crosslingual', 'crossstitch', 'crowe', 'cruft', 'cs', 'csplit', 'ct', 'ct1', 'cups', 'curl', 'current', 'curve', 'curves', 'cut', 'cute', 'cvpr', 'd1', 'd2', 'daemon', 'dan', 'daniel', 'danihelka', 'darrell', 'dat', 'data', 'dataset', 'datasets', 'date', 'daum', 'david', 'day', 'dbg', 'dc', 'dd', 'ddrescue', 'deals', 'dean', 'deattach', 'debind', 'debra', 'debug', 'decay4', 'decide', 'decipher', 'decision', 'declare', 'decoder', 'decomposes', 'decompress', 'deep', 'defacto133', 'default', 'define', 'defined', 'degrees', 'delay', 'delete', 'dell', 'demonstrate', 'demonstrated', 'demonstrates', 'demonstrating', 'demote', 'denmark', 'denote', 'densely', 'densenet', 'deoras', 'dependencies', 'dependency', 'depends', 'depp', 'depth', 'depths', 'der', 'derive', 'derived', 'dernoncourt', 'descent', 'described', 'design', 'desirable', 'desktop', 'despite', 'detailed', 'details', 'detect', 'detected', 'detection', 'determine', 'deterministic', 'dev', 'devede', 'develop', 'developed', 'developing', 'development', 'device', 'df', 'dh', 'dharo', 'dhillon', 'di', 'dialog', 'dialogs', 'dialogue', 'dialogues', 'dictionary', 'did', 'didn', 'didnt', 'diff', 'diff3', 'difference', 'differences', 'different', 'differentiate', 'differs', 'difficult', 'difficulty', 'dilip', 'dim', 'dimension', 'dimensional', 'dimensionality', 'dimensions', 'dinh', 'dir', 'dircolors', 'direction', 'directly', 'dirname', 'dirs', 'dirtiness', 'dirty', 'disable', 'discourage', 'discourse', 'discover', 'discover_activity', 'discovered', 'discrete', 'discriminative', 'discussed', 'discusses', 'discussion', 'distance', 'distributed', 'distribution', 'diverse', 'diversity', 'divided', 'division', 'dk', 'dm', 'dmed', 'dmesg', 'dmraid', 'docs', 'document', 'does', 'doesn', 'doesnt', 'doing', 'dolan', 'domain', 'domains', 'domainspecific', 'don', 'dont', 'dot', 'download', 'downloaded', 'downtown', 'downwards', 'dpkg', 'dramatic', 'draw', 'dredze', 'driven', 'driver', 'drivers', 'drives', 'drop', 'dropout', 'dsh', 'dsl', 'dstc', 'dtic', 'du', 'dublin', 'ducharme', 'dudz', 'dumitru', 'dump', 'duong', 'duplicating', 'dyer', 'dynamic', 'dyndns', 'dynet', 'eacl', 'earlier', 'early', 'easily', 'easy', 'echo', 'ecryptfs', 'ed', 'editor', 'edu', 'eduard', 'edward', 'edwards', 'effect', 'effective', 'effectively', 'effectiveness', 'effects', 'efficient', 'egrep', 'eighty', 'eisenstein', 'eject', 'ek', 'el', 'element', 'elements', 'elementwise', 'elizabeth', 'ellis', 'email', 'embedding', 'embeddingbased', 'embeddings', 'emma', 'emnlp', 'emoticons', 'emphasis', 'empirical', 'empirically', 'empiricial', 'employ', 'employed', 'employs', 'en', 'enable', 'enables', 'enabling', 'encoded', 'encoder', 'encoderdecoder', 'encoders', 'encodes', 'encoding', 'encouraged', 'end', 'ending', 'enforce', 'enforced', 'engineered', 'english', 'enjoy', 'enjoyed', 'enjoying', 'ensure', 'ensures', 'ent', 'enter', 'entering', 'entertainment', 'entire', 'entities', 'entity', 'entropy', 'env', 'environment', 'epfl', 'epiphany', 'epochs', 'equally', 'equation', 'equinix', 'equivalent', 'erhan', 'erin', 'error', 'errors', 'esmtp', 'especially', 'essdykema', 'established', 'estimate', 'estimation', 'et', 'etwork', 'etworks', 'etzioni', 'european', 'eurospeech', 'eval', 'evaluate', 'evaluating', 'evaluation', 'evaluators', 'everybody', 'evgeniou', 'evolves', 'ew', 'ex', 'exact', 'example', 'examples', 'exception', 'excerpt', 'excited', 'excluding', 'exclusive', 'exec', 'executable', 'execute', 'exhaustive', 'exhibits', 'exist', 'existing', 'exists', 'exit', 'exp', 'expand', 'expect', 'experience', 'experiment', 'experimental', 'experimented', 'experiments', 'expert', 'explain', 'explanation', 'explicit', 'explicitly', 'exploding', 'exploit', 'exploiting', 'exploits', 'exploration', 'explore', 'explored', 'export', 'expr', 'extends', 'extent', 'extract', 'extracted', 'extracting', 'extraction', 'extracts', 'extrema', 'f1', 'fabbione', 'face', 'fact', 'factor', 'factorization', 'factorized', 'factorizing', 'fail', 'failed', 'false', 'fan', 'far', 'fashion', 'fast', 'faster', 'fatrat', 'fbdesk', 'fdformat', 'feature', 'features', 'fed', 'feed', 'feedback', 'feedforward', 'feel', 'fellow', 'feng', 'fernando', 'fetch', 'fever', 'ff', 'fg', 'fglrx', 'fgrep', 'fields', 'fifth', 'figure', 'file', 'files', 'filter', 'filters', 'final', 'finally', 'finding', 'findings', 'fine', 'finger', 'firat', 'firefox', 'first_speaker', 'fit', 'fitting', 'fix', 'fixed', 'fixing', 'flexible', 'flow', 'fluency', 'fluent', 'fmt', 'focus', 'focused', 'fold', 'followed', 'following', 'follows', 'force', 'forces', 'foremost', 'form', 'formally', 'format', 'formulas', 'formulation', 'forsyth', 'forth', 'forum', 'forward', 'fourth', 'framework', 'franchini', 'francis', 'franck', 'freedom', 'freeze', 'frequent', 'fresh', 'frobenius', 'frustratingly', 'fsck', 'fstab', 'ft', 'ftp', 'fullscreen', 'function', 'functionality', 'functions', 'furthermore', 'fuser', 'future', 'g1', 'ga', 'gaining', 'gains', 'galley', 'galway', 'game', 'gang', 'gao', 'gasic', 'gated', 'gateway', 'gateways', 'gating', 'gaussian', 'gave', 'gawk', 'gcc', 'gconf', 'gedit', 'geertzen', 'geertzen2007', 'gelbart', 'general', 'generalization', 'generalize', 'generalizing', 'generally', 'generate', 'generated', 'generates', 'generating', 'generation', 'generative', 'generatively', 'generic', 'george', 'georgila', 'get_activity', 'getopts', 'gets', 'getting', 'gi', 'girlfriend', 'github', 'given', 'gives', 'giving', 'gk', 'glad', 'glove', 'gm', 'gnome', 'gnomeradio', 'goal', 'goel', 'goes', 'going', 'gold', 'goldberg', 'gone', 'gonna', 'good', 'google', 'gordons', 'got', 'gotten', 'gpac', 'grab', 'gradient', 'grained', 'graph', 'graphical', 'graphically', 'graves', 'great', 'greedy', 'grefenstette', 'greg', 'grep', 'ground', 'group', 'groupadd', 'groupdel', 'grouping', 'groupmod', 'groups', 'growing', 'gru', 'gsfonts', 'gtesauro', 'guaranteed', 'guarantees', 'gulcehre', 'gulrajani', 'gupta', 'gutenprint', 'guys', 'gym', 'gzip', 'h0', 'h1', 'ha', 'ha1', 'haffari', 'haha', 'hahahahahahahaha', 'hal', 'half', 'hand', 'handwriting', 'hang', 'hannah', 'happens', 'happy', 'hard', 'hardly', 'hardt', 'hardware', 'harm', 'harry', 'hash', 'hashimoto', 'hasn', 'haven', 'havent', 'having', 'hb', 'hb2', 'head', 'heads', 'headwords2', 'heart', 'heat', 'heavy', 'hebert', 'hed', 'height', 'heights', 'help', 'helping', 'helps', 'henderson', 'henrik', 'heres', 'hes', 'heterogeneous', 'hey', 'hi', 'hibernate', 'hid', 'hidden', 'hierarchical', 'high', 'higher', 'highest', 'highly', 'hinton', 'history', 'hither', 'hlt', 'hm', 'hmm', 'hmms', 'hn', 'hn1', 'hochreiter', 'hoi', 'hold', 'home', 'homogeneous', 'hospedales', 'hostname', 'hotwire', 'hours', 'house', 'hovy', 'howbeit', 'hows', 'hred', 'ht', 'ht1', 'htop', 'http', 'https', 'huang', 'hugs', 'human', 'humans', 'hundreds', 'hung', 'huval', 'hyper', 'hyperbolic', 'hyperparameter', 'hyperparameters', 'hypothesis', 'hypothesize', 'i364', 'i5', 'ia32', 'ibm', 'icassp', 'icassp03', 'iclr', 'icml', 'iconv', 'icsc', 'icsi', 'id', 'idea', 'identical', 'identified', 'identify', 'identifying', 'ieee', 'ifconfig', 'ifdown', 'ifenslave', 'ifplugd', 'ifup', 'ignore', 'ii', 'iii', 'ill', 'illustrated', 'illustrator', 'ilya', 'im', 'image', 'immediate', 'impact', 'imperative', 'implement', 'implemented', 'implicit', 'implicitly', 'imply', 'import', 'importance', 'important', 'importantly', 'impose', 'impressive', 'improve', 'improved', 'improvement', 'improvements', 'improves', 'inability', 'include', 'includes', 'including', 'incorporates', 'incorporating', 'increase', 'increased', 'increases', 'increasing', 'index', 'indicate', 'indicated', 'indicates', 'indicating', 'indicator', 'individual', 'indomain', 'induce', 'inducing', 'inductive', 'inetd', 'infeasible', 'inferior', 'info', 'information', 'informational', 'informative', 'informed', 'informs', 'inherent', 'initial', 'initializations', 'initialize', 'initialized', 'inner', 'innermost', 'inotify', 'input', 'inputs', 'inside', 'inspecting', 'inspection', 'inspired', 'install', 'installed', 'installing', 'instances', 'instead', 'instructed', 'instructions', 'intelligence', 'intended', 'inter', 'interact', 'interaction', 'interactions', 'interlabeler', 'internal', 'international', 'internet', 'interrupt', 'intra', 'introduce', 'introduced', 'introducing', 'inui', 'invention', 'investigate', 'involving', 'inward', 'io', 'ip', 'iptables', 'irc', 'ireland', 'irq', 'irregular', 'irrespective', 'irsoy', 'irssi', 'isabelle', 'isn', 'isnt', 'isolation', 'issues', 'italic', 'itd', 'ith', 'itll', 'iulian', 'iulianserban', 'iv', 'ive', 'iwsds', 'jacob', 'jair', 'jalali', 'jamuary', 'jan', 'jane', 'janin', 'janvin', 'jason', 'java', 'jb', 'jean', 'jeff', 'jeffrey', 'jeremy', 'jeroen', 'ji', 'jian', 'jieping', 'jin', 'jk', 'jmlr', 'joachim', 'joao', 'jobs', 'john', 'johnny', 'join', 'joint', 'jointly', 'journal', 'jpineau', 'jrgen', 'judgment', 'julianser', 'jun', 'june', 'junior', 'jurafsky', 'just', 'justified', 'justin', 'jwrigley', 'k2f', 'kai', 'kalchbrenner', 'karlen', 'kaski', 'kastner', 'kaufman', 'kavukcuoglu', 'kde', 'kdelibs', 'keeping', 'keeps', 'kentaro', 'kept', 'kernel', 'key', 'keys', 'kg', 'kid3', 'kids', 'kilian', 'kill', 'killall', 'kim', 'kim2014', 'kind', 'kingma', 'klami', 'klaus', 'klesen', 'klesen1997', 'klinger', 'km', 'kn', 'knew', 'know', 'knowledge', 'knowledgebased', 'known', 'knows', 'koehn', 'kombrink', 'koray', 'krishnan', 'krtalamad', 'ku', 'kuksa', 'kuldip', 'kumar', 'kurohashi', 'kvm', 'kyunghyun', 'l0', 'l1', 'l2', 'la', 'lab', 'label', 'labeling', 'labelled', 'labels', 'labor', 'lance', 'language', 'languages', 'large', 'largely', 'larger', 'lasso', 'lately', 'latent', 'later', 'latest', 'launch', 'laurens', 'layer', 'layer3', 'layered', 'layers', 'layerwise', 'ld', 'ldc2013t19', 'ldv', 'le', 'lead', 'learn', 'learnable', 'learned', 'learning', 'learns', 'leave', 'lee', 'left', 'lendvai', 'length', 'lest', 'let', 'lets', 'letting', 'level', 'levels', 'leverage', 'lewandowski', 'lexical', 'li', 'lib', 'libc', 'libjpeg', 'libpulse', 'library', 'libreoffice', 'libs', 'libsane', 'libssl', 'light', 'like', 'liked', 'likelihood', 'likely', 'lilypond', 'limit', 'limitations', 'limited', 'line', 'linear', 'linearly', 'linger', 'lingual', 'linguistics', 'link', 'linux', 'lioma', 'list', 'lists', 'literature', 'little', 'liu', 'lk', 'll', 'lm', 'ln', 'load', 'local', 'locate', 'lock', 'lockout', 'log', 'login', 'loglikelihood', 'logname', 'logout', 'logs', 'lol', 'long', 'look', 'looking', 'looks', 'loose', 'loosely', 'loss', 'losses', 'lot', 'love', 'loved', 'low', 'lowe', 'lower', 'lowercases', 'lowest', 'lozano', 'lpc', 'lpr', 'lprm', 'ls', 'lsb', 'lsmt', 'lsof', 'lstm', 'lubuntu', 'luice', 'luis', 'lukar', 'luong', 'lusa', 'lxde', 'm1', 'maaten', 'mac', 'macaddress', 'macbook', 'machine', 'machines', 'magazine', 'main', 'mainly', 'maintain', 'maintaining', 'maintains', 'majority', 'make', 'makefile', 'makes', 'making', 'man', 'management', 'manager', 'managers', 'manning', 'manning2012', 'manual', 'manually', 'map', 'mapped', 'mapping', 'maps', 'mar', 'marcus', 'margin', 'marginally', 'marie', 'mark', 'markov', 'marks', 'martell', 'martellwhich', 'martha', 'martin', 'martnez', 'massimiliano', 'master', 'mate', 'mathematical', 'matrices', 'matrix', 'matrixvector', 'matter', 'matthew', 'maurer', 'mausam', 'max', 'maximize', 'maximizing', 'maximum', 'maybe', 'mcgill', 'mdf2iso', 'mdk', 'mdm', 'mean', 'means', 'meantime', 'measure', 'measured', 'measuring', 'mechanism', 'mechanisms', 'media', 'mediated', 'mediating', 'meeting', 'meetings', 'members', 'memory', 'mentioned', 'merely', 'merged', 'merrienboer', 'message', 'messing', 'meta', 'meteer', 'meteor', 'method', 'methods', 'metrics', 'mg', 'michael', 'michelle', 'middle', 'mihai', 'mike', 'mikolov', 'million', 'minimal', 'minimization', 'minimize', 'minimizing', 'minimum', 'mirage', 'mirror', 'misclassifications', 'misra', 'miss', 'misspelled', 'mit', 'mitchell', 'mixture', 'mixtures', 'mixxx', 'mkdir', 'mkfifo', 'mknod', 'ml', 'mlp', 'mmv', 'mnt', 'model', 'modeled', 'modeling', 'modelling', 'models', 'module', 'modules', 'moment', 'monitor', 'montreal', 'monz', 'morgan', 'moritz', 'moses', 'mosesdecoder', 'motivated', 'mount', 'mounted', 'movie', 'mp3check', 'mr', 'mrda', 'mrksic', 'mrrnn', 'mrrnns', 'mrs', 'mtl', 'mtools', 'mtr', 'mug', 'multi', 'multidomain', 'multilevel', 'multiparty', 'multiple', 'multiplication', 'multiplied', 'multiresolution', 'multitask', 'music', 'mute', 'mv', 'mz', 'n0', 'n01', 'n1', 'n10', 'n100', 'n10658', 'n10916', 'n11', 'n112', 'n11200', 'n11274', 'n11846', 'n12', 'n1292', 'n12k', 'n13', 'n1306', 'n1357', 'n1367', 'n14', 'n147955', 'n15', 'n15421', 'n16', 'n164217', 'n16734', 'n17', 'n1780', 'n17874', 'n18', 'n1869', 'n19', 'n1997', 'n1998', 'n2', 'n20', 'n200', 'n2000', 'n2004', 'n2007', 'n2011', 'n2012', 'n2015', 'n2016', 'n2017', 'n20k', 'n21', 'n2112', 'n21520', 'n22', 'n2206', 'n2281', 'n2297', 'n2326', 'n24', 'n25', 'n25206', 'n25271', 'n25883', 'n26', 'n26424', 'n27', 'n28', 'n287', 'n29', 'n2920', 'n297049', 'n29957', 'n3', 'n33', 'n34', 'n34944', 'n35', 'n35947', 'n36', 'n37', 'n38', 'n388851', 'n39', 'n4', 'n40', 'n43', 'n45', 'n49', 'n49393', 'n5', 'n50', 'n500', 'n51', 'n52', 'n52225', 'n53', 'n57', 'n5893', 'n59', 'n6', 'n60756', 'n63', 'n64', 'n641', 'n65', 'n66', 'n67', 'n68', 'n69', 'n6905', 'n6k', 'n7', 'n70', 'n71', 'n72', 'n73', 'n74', 'n779', 'n786794', 'n8', 'n81', 'n82', 'n83', 'n84', 'n85', 'n86', 'n87', 'n878223', 'n88', 'n89', 'n8th', 'n9', 'n90', 'n90403', 'n9094', 'n91', 'n92', 'n93', 'n94', 'n95', 'n96', 'n97', 'n98', 'na', 'na1', 'naa', 'naacl', 'naacl2016', 'naaron', 'nabhishek', 'nability', 'nablation', 'nabout', 'nabramov', 'nabstract', 'nabstraction', 'nacc', 'naccept', 'naccess', 'naccident', 'naccording', 'naccuracy', 'nachieve', 'nacknowledgments', 'nacpid_entity', 'nacross', 'nact', 'nactentrepresentation', 'nactivities', 'nactivity', 'nacts', 'nadadelta', 'nadding', 'naddition', 'nadditional', 'nadds', 'nadept', 'nadvances', 'nadversarial', 'naffirm', 'nafk', 'nafterwards', 'nagainst', 'nagreement', 'naive', 'nakagawa', 'nal', 'nali', 'nalias', 'nalignment', 'nall', 'nallowed', 'nallows', 'nalma', 'nalong', 'nalready', 'nalso', 'namd64', 'namd64_entity', 'named', 'namong', 'nan', 'nanalysis', 'nand', 'nanders', 'nandreas', 'nannotation', 'nannual', 'nano', 'nanoop', 'nanother', 'nany', 'nanyways', 'napartment', 'nappendix', 'napplication', 'napply', 'napt', 'narchitecture', 'narchitectures', 'nare', 'narg', 'naround', 'nartificial', 'narxiv', 'nas', 'nassigned', 'nassociation', 'nat', 'nathan', 'national', 'nattention', 'natural', 'naturally', 'naugmented', 'nautilus', 'nauxiliary', 'naverage', 'navg', 'nay', 'naylien', 'nazureus', 'nb', 'nba', 'nbackoff', 'nbadly', 'nbarbara', 'nbased', 'nbaseline', 'nbaselines', 'nbathroom', 'nbayesian', 'nbc', 'nbe', 'nbeen', 'nbeneficial', 'nbeside', 'nbetter', 'nbetween', 'nbias', 'nbif', 'nbigram', 'nbless', 'nblob', 'nbn', 'nboost', 'nboth', 'nbowen', 'nbroadcast', 'nbrowsing', 'nbuilding', 'nbundle', 'nbut', 'nby', 'nc', 'ncan', 'ncar', 'ncertain', 'nchat', 'ncheck_activity', 'ncheckout', 'nchkconfig', 'nchoice', 'nchristian', 'nchristopher', 'nchunk', 'nchunking', 'ncifar', 'nclass', 'nclassification', 'nclassifying', 'nclosely', 'nclusters', 'ncmd', 'ncmpcpp', 'ncnn', 'ncoarse', 'ncoccaro', 'ncoders', 'ncome', 'ncomments', 'ncommunication', 'ncompared', 'ncomplexity', 'ncomputational', 'ncomputed', 'ncomputing', 'nconcatenation', 'nconclusion', 'nconference', 'nconfirming', 'nconnected', 'nconsciously', 'nconsidered', 'nconsist', 'nconsistently', 'nconsisting', 'nconstant', 'ncontext', 'ncontributions', 'ncontrol', 'nconv', 'nconversation', 'nconversational', 'nconversations', 'nconvolutional', 'ncorpus', 'ncorr', 'ncorrado', 'ncorrelating', 'ncorresponding', 'ncorresponds', 'ncough', 'ncould', 'ncross', 'nct', 'nct1', 'ncurious', 'nd', 'nd1', 'nd2', 'ndan', 'ndata', 'ndataset', 'ndatasets', 'ndeactivate', 'ndebian', 'ndeep', 'ndeliver', 'ndemonstrate', 'ndepartment', 'ndescent', 'ndesignates', 'ndestroy', 'ndev', 'ndialogue', 'ndifferent', 'ndig', 'ndiscourse', 'ndiscrete', 'ndiscussion', 'ndisplay', 'ndisregard', 'ndistinct', 'ndistributed', 'ndistributions', 'ndiswrapper', 'ndivided', 'ndo', 'ndomain', 'ndomains', 'ndominates', 'ndon', 'ndont', 'ndownload', 'ndropout', 'ndstc', 'ndstc4', 'ndt', 'ndue', 'ndzmitry', 'ne', 'neach', 'near', 'nearly', 'neasily', 'nec', 'necessarily', 'necessary', 'neduard', 'need', 'needs', 'neffective', 'negative', 'nelson', 'nemail', 'nembedding', 'nemma', 'nemnlp', 'nenables', 'nenabling', 'nencode', 'nencoder', 'nentire', 'nentities', 'nentity', 'ner', 'nerd', 'net', 'netball', 'netc', 'netcfg', 'netciptables', 'nets', 'network', 'networks', 'neubig', 'neural', 'nevaluate', 'nevaluation', 'nevaluators', 'new', 'news', 'newswire', 'nexact', 'nexception', 'nexisting', 'nexp', 'nexperiment', 'nexperiments', 'nexplore', 'nexpressions', 'nextending', 'nextracted', 'nextracting', 'nextrema', 'nf', 'nf1', 'nfailed', 'nfdisk', 'nff1', 'nff2', 'nfigure', 'nfind', 'nfine', 'nfirst', 'nfish', 'nfix', 'nfixed', 'nflows', 'nfluency', 'nfor', 'nformer', 'nforsyth', 'nfranck', 'nfrancky', 'nfrom', 'nfrustratingly', 'nft', 'nfuture_tenses', 'ng', 'ngao', 'ngeertzen', 'ngeneral', 'ngeneralization', 'ngeneralize', 'ngeneralizes', 'ngenerate', 'ngenerated', 'ngeneration', 'ngerald', 'ngi', 'ngithub', 'ngive', 'ngiven', 'nglobal', 'ngm', 'ngo', 'ngoal', 'ngoldberg', 'ngonna', 'ngradient', 'ngradients', 'ngraham', 'ngraphical', 'ngreedy', 'ngrefenstette', 'ngroup', 'ngrzegorz', 'nh', 'nha', 'nha1', 'nha2', 'nhad', 'nhal', 'nhandbook', 'nhanging', 'nhard', 'nhardware', 'nhash', 'nhashimoto', 'nhave', 'nhb', 'nhctor', 'nhello', 'nher', 'nhere', 'nherein', 'nheterogeneous', 'nhey', 'nhi', 'nhidden', 'nhierarchical', 'nhierarchy', 'nhigh', 'nhighlight', 'nhis', 'nhmm', 'nhn', 'nhouston', 'nhow', 'nhowever', 'nhred', 'nht', 'nhttp', 'nhttps', 'nhuman', 'nhyperparameter', 'ni', 'ni5', 'nianwen', 'nibm', 'nicml', 'nids', 'nie', 'nif', 'niii', 'nikto', 'nill', 'nim', 'nimmediately', 'nimportance', 'nimportant', 'nin', 'ninclude', 'nindependent', 'nindex', 'nindicate', 'ninductive', 'ninety', 'ninformation', 'ninitial', 'ninitialization', 'ninput', 'ninsight', 'ninspection', 'ninspired', 'ninstall', 'ninstall_activity', 'ninstitute', 'ninstructions', 'ninterlabeler', 'ninterspeech', 'ninto', 'nintroduce', 'nintroduction', 'ninvestigate', 'nips', 'niptables', 'nis', 'nishan', 'nissan', 'nisupportwomeninsport', 'nit', 'niulian', 'nive', 'nj', 'njean', 'nji', 'njjylee', 'njoachim', 'njockey', 'njoelle', 'njointly', 'njonathan', 'njournal', 'njurafsky', 'njurgen', 'njust', 'nk', 'nkai', 'nkaiming', 'nkalchbrenner', 'nkartik', 'nkazuma', 'nkeeping', 'nkgm', 'nkids', 'nklesen', 'nkn', 'nknow', 'nknowledge', 'nkonstantinos', 'nl', 'nlack', 'nlanguage', 'nlanguages', 'nlasso', 'nlaurent', 'nlayer', 'nlayers', 'nlazarus', 'nleads', 'nlearn', 'nlearned', 'nlearning', 'nlee', 'nlength', 'nleon', 'nlet', 'nli', 'nlibfreetype6', 'nlibreoffice', 'nlike', 'nlikely', 'nlinguistic', 'nlinguistics', 'nlinux', 'nlist', 'nlittle', 'nliu', 'nll', 'nln', 'nlocate', 'nlog', 'nlong', 'nlonger', 'nloosely', 'nloss', 'nlow', 'nlower', 'nlp', 'nlsb_entity', 'nlstm', 'nm', 'nmagazines', 'nmajority', 'nmake', 'nmanager', 'nmanning', 'nmanual', 'nmanually', 'nmany', 'nmargin', 'nmartial', 'nmartin', 'nmathieu', 'nmatrix', 'nmatthews', 'nmax', 'nmaximizing', 'nmaybe', 'nmeaningful', 'nmediated', 'nmemory', 'nmendes', 'nmeta', 'nmethod', 'nmetrics', 'nmight', 'nmiguel', 'nmisra', 'nmisspelled', 'nmit', 'nmixture', 'nmodel', 'nmodeling', 'nmodels', 'nmohammed', 'nmontral', 'nmonz', 'nmore', 'nmoses', 'nmost', 'nmotivated', 'nmount', 'nmrda', 'nmrrnn', 'nmulti', 'nmultilingual', 'nmultiple', 'nmultiresolution', 'nmy', 'nmz', 'nn', 'nnaive', 'nnal', 'nnamed', 'nnanyun', 'nnatural', 'nnavigate', 'nnecessary', 'nner', 'nnetwork', 'nnetworks', 'nneural', 'nnew', 'nnews', 'nnewswire', 'nnips', 'nnn', 'nnnp', 'nno', 'nno_cmd', 'nno_tenses', 'nnohup', 'nnoise', 'nnon', 'nnorm', 'nnot', 'nnote', 'nnoted', 'nnoun', 'nnouns', 'nnumber', 'nnw', 'nny', 'no_cmd', 'no_nouns', 'no_tenses', 'noah', 'nobjective', 'nof', 'noise', 'noisy', 'non', 'none_activity', 'nones', 'nonetheless', 'nonly', 'nontonotes', 'nood', 'nopen', 'nopposed', 'norbert', 'norder', 'norhan', 'norm', 'normally', 'norms', 'north', 'nos', 'noseworthy', 'notation', 'note', 'nother', 'notherwise', 'noticing', 'notify', 'noun', 'nouns', 'nour', 'nours', 'nout', 'nouter', 'noutperforms', 'noutput', 'noutputs', 'noutset', 'novel', 'nover', 'noverall', 'noverfitting', 'np', 'npages', 'npairs', 'npaliwal', 'nparallel', 'nparameter', 'nparameters', 'nparametrization', 'npart', 'nparticular', 'npartitioned', 'npast_present_tenses', 'npatience', 'npenalty', 'npeng', 'npengfei', 'nper', 'nperfect', 'nperformance', 'npg', 'nph', 'npinkberry', 'npivot', 'npk', 'nplaceholder', 'nplank', 'npooling', 'npos', 'nposition', 'npotentially', 'nprediction', 'npredictions', 'npredictive', 'npreferable', 'npreprint', 'npreprocessing', 'npresent_future_tenses', 'npresent_tenses', 'npresents', 'nprevious', 'nprincess', 'nprior', 'nproblem', 'nproblems', 'nprocedure', 'nproceedings', 'nprocessing', 'nprofit', 'npromising', 'npropose', 'nproposed', 'nproud', 'nprovided', 'nps', 'npt', 'npublicly', 'npurge', 'npurposes', 'nr', 'nrafael', 'nralph', 'nramshaw', 'nrandomly', 'nrar', 'nravikumar', 'nreal', 'nreboot', 'nrecall', 'nrecent', 'nrecht', 'nrecognition', 'nrecurrent', 'nreduce', 'nreductions', 'nredundancy', 'nreferences', 'nrefers', 'nregularization', 'nregularized', 'nregularizers', 'nrelated', 'nrelatedness', 'nrelation', 'nrelatively', 'nrelevancy', 'nrelevant', 'nrepresentation', 'nrepresentations', 'nresearch', 'nresearchers', 'nrespectively', 'nresponse', 'nresponses', 'nrestricted', 'nresults', 'nrich', 'nrnn', 'nrnnlm', 'nromera', 'nronan', 'nrow', 'nrule', 'nrussell', 'ns', 'ns2v', 'nsame', 'nsampling', 'nschmidhuber', 'nscripts', 'nsearch', 'nsebastian', 'nsecond', 'nsection', 'nsee', 'nseeming', 'nselective', 'nsell', 'nsemantic', 'nsenna', 'nsensors', 'nsentence', 'nsentiment', 'nsepp', 'nseppo', 'nsequence', 'nsequences', 'nsequential', 'nserban', 'nseriously', 'nsetting', 'nseveral', 'nsgaard', 'nsgd', 'nshallow', 'nshare', 'nshared', 'nsharing', 'nshort', 'nshow', 'nshows', 'nshriberg', 'nshuiwang', 'nsignificantly', 'nsimilar', 'nsimilarly', 'nsimplified', 'nsince', 'nsingle', 'nsketched', 'nskip', 'nslookup', 'nsluice', 'nsocher', 'nsome', 'nsometimes', 'nsonali', 'nsong', 'nsorry', 'nsort', 'nsoundconverter', 'nspaces', 'nspecial', 'nspeech', 'nspray', 'nsrl', 'nstand', 'nstatistics', 'nstress', 'nstrong', 'nstudy', 'nstuff', 'nsub', 'nsubspace', 'nsubspaces', 'nsubstantial', 'nsuccessfully', 'nsuch', 'nsudo', 'nsufficient', 'nsuited', 'nsupervision', 'nsvm', 'nswda', 'nsynaptic', 'nsynonyms', 'nsystem', 'nsystems', 'nt', 'ntab', 'ntable', 'ntagger', 'ntagging', 'ntakes', 'ntanh', 'ntask', 'ntasks', 'ntc', 'ntechnical', 'ntelephone', 'ntense', 'ntenses', 'nterm', 'ntest', 'ntexts', 'nth', 'nthan', 'nthank', 'nthat', 'nthe', 'ntheir', 'nthemselves', 'ntheory', 'ntherefore', 'nthereve', 'nthese', 'nthey', 'nthing', 'nthis', 'nthough', 'nthree', 'nthrough', 'nthru', 'ntim', 'ntime', 'nto', 'ntoken', 'ntokenizer', 'ntokens', 'ntopics', 'ntrain', 'ntrainable', 'ntraining', 'ntransfer', 'ntranslate', 'ntranslation', 'ntrevor', 'ntrue', 'ntwitter', 'ntwo', 'nubuntu', 'nubuntu_7', 'nubuntudialoguecorpus', 'nuemulator', 'nugh', 'num', 'number', 'numbers', 'nunidir', 'nunigram', 'nunion', 'nunits', 'nuniversity', 'nunlike', 'nunmount', 'nupdates', 'nupgrade', 'nupgrade_activity', 'nuse', 'nused', 'nuses', 'nusing', 'nutterance', 'nutterances', 'nuuencode', 'nuwidget', 'nv', 'nvalidation', 'nvbd', 'nvectors', 'nversion', 'nvery', 'nvhred', 'nvidia', 'nviews', 'nvlc', 'nvols', 'nw', 'nw1', 'nwas', 'nwb', 'nwd', 'nwe', 'nweblogs', 'nweight', 'nweighted', 'nweights', 'nwell', 'nwhat', 'nwhats', 'nwhen', 'nwhere', 'nwhich', 'nwhichever', 'nwhile', 'nwhither', 'nwho', 'nwi', 'nwilsonphillips', 'nwith', 'nwithin', 'nwj', 'nwn', 'nword', 'nwords', 'nwork', 'nwork_activity', 'nworks', 'nwww', 'nx', 'nxue', 'ny', 'nyang', 'nyeah', 'nyj', 'nyongxin', 'nyorktown', 'nyoshua', 'nyou', 'nz', 'nzbget', 'nzhilin', 'nzi', 'nzip', 'nzn', 'objective', 'observation', 'observe', 'observed', 'observes', 'observing', 'obtain', 'obtained', 'obviously', 'occur', 'occurrence', 'ofdomain', 'offtopic', 'oh', 'ok', 'okay', 'old', 'omg', 'omitted', 'omitting', 'onboard', 'ondruska', 'ones', 'online', 'ontonotes', 'ontoppic', 'ood', 'op', 'open', 'opens', 'operating', 'operation', 'operations', 'optimization', 'optimize', 'optimizing', 'ord', 'order', 'ordering', 'ords', 'oriented', 'oriol', 'orthogonal', 'orthogonality', 'oscar', 'ought', 'outer', 'outperform', 'outperformed', 'outperforms', 'output', 'outputs', 'outside', 'overall', 'overcome', 'overcoming', 'overfitting', 'overheat', 'overlap', 'overlapping', 'overlayscrollbar', 'overview', 'ovt', 'owing', 'owoputi', 'package', 'page', 'pages', 'painting', 'pair', 'pairing', 'pairs', 'paliwal', 'paliwal1997', 'palmer', 'pan', 'pane', 'paper', 'papers', 'par', 'parallel', 'param', 'parameter', 'parameters', 'parametrization', 'parametrize', 'parametrized', 'paraphrased', 'paraphrases', 'paredes', 'parenthesis', 'parser', 'parsing', 'partial', 'partially', 'participating', 'particular', 'particularly', 'partition', 'partitioning', 'partly', 'parts', 'pascanu', 'passed', 'passwd', 'past', 'paste', 'pastebin', 'patch', 'paths', 'patience', 'paul', 'pavel', 'pc', 'penalty', 'peng', 'pennington', 'people', 'percentage', 'perceptrons', 'perform', 'performance', 'performances', 'performs', 'perl', 'perplexity', 'person', 'persuasive', 'peskin', 'pfau', 'phenomena', 'phil', 'philippe', 'photoshop', 'phrase', 'phrases', 'pidgin', 'pierce', 'pilot', 'pineau', 'ping', 'pink', 'pinkberry', 'piroska', 'pittsburgh', 'pkill', 'place', 'placed', 'placeholder', 'plan', 'plank', 'plateau', 'platform', 'play', 'playback', 'player', 'plenty', 'plot', 'plug', 'plus', 'pmount', 'point', 'pointer', 'points', 'polyphonic', 'pomdp', 'pontil', 'pooling', 'poorly', 'popd', 'popularity', 'portions', 'pos', 'possible', 'possibly', 'post', 'posteriori', 'potential', 'potentially', 'potter', 'pow', 'pp', 'ppa', 'pppoe', 'pr', 'practical', 'practice', 'pradeep', 'pradhan', 'pragmatic', 'pre', 'preceding', 'precious', 'precision', 'predefined', 'predefining', 'predict', 'predicted', 'predicting', 'prediction', 'predictions', 'predictive', 'predictors', 'predicts', 'predominantly', 'prefer', 'prefix', 'preliminary', 'prepositions', 'prepping', 'preprint', 'preprocess', 'preprocessed', 'preprocessing', 'prereg', 'present', 'presented', 'presents', 'pretrained', 'pretty', 'prevent', 'previous', 'previously', 'primarily', 'princess', 'principal', 'principle', 'print', 'printf', 'prints', 'priori', 'priors', 'private', 'pro', 'probabilistic', 'probabilities', 'probability', 'probably', 'problem', 'problems', 'proc', 'procedure', 'procedures', 'proceedings', 'process', 'processed', 'processes', 'processing', 'produce', 'produced', 'produces', 'product', 'profit', 'proftpd', 'program', 'programming', 'progresses', 'project', 'projected', 'projection', 'promising', 'promoting', 'promptly', 'prone', 'pronounced', 'pronouns', 'propagate', 'properties', 'property', 'proposals', 'propose', 'proposed', 'proposing', 'pros', 'prose', 'provide', 'provided', 'provides', 'providing', 'ps', 'ptb', 'public', 'publication', 'publicly', 'publish', 'pull', 'pulseaudio', 'punctuation', 'pure', 'purge', 'pursue', 'push', 'pushd', 'pv', 'pwd', 'python', 'qc', 'qemu', 'qiu', 'quadratically', 'qualitative', 'quality', 'quantitative', 'que', 'queries', 'query', 'question', 'questions', 'quickly', 'quite', 'quota', 'quotacheck', 'quotactl', 'quote', 'qv', 'r2d', 'r44', 'rachel', 'rademacher', 'radio', 'railscamp', 'raj', 'ralph', 'ram', 'ramshaw', 'ran', 'random', 'randomly', 'randomness', 'range', 'ranges', 'ranging', 'raquel', 'rare', 'raring', 'rate', 'rated', 'rating', 'ratings', 'ratio', 'rcp', 'rd', 'rdh', 'read', 'reader', 'readily', 'reading', 'readonly', 'real', 'realized', 'really', 'reason', 'reasonable', 'reasons', 'reattach', 'reba', 'rebecca', 'reboot', 'recall', 'receive', 'receives', 'recent', 'recently', 'recognition', 'recommendations', 'recorder', 'recruit', 'rectified', 'recurrent', 'recursive', 'redirect', 'redistribute', 'reduction', 'reductions', 'ref', 'refer', 'reference', 'refers', 'reflect', 'refresh', 'refs', 'regarding', 'regardless', 'regards', 'regression', 'regular', 'regularised', 'regularization', 'regularized', 'regularizer', 'regularizers', 'regularizing', 'reinstall', 'reithinger', 'reject', 'relabelings', 'relate', 'related', 'relation', 'relations', 'relayed', 'release', 'relevancy', 'relevant', 'relies', 'relu', 'rely', 'remain', 'remaining', 'remains', 'remake', 'remember', 'reminds', 'remove', 'removed', 'removing', 'ren', 'rename', 'repeat', 'repeated', 'repeating', 'replace', 'replaced', 'report', 'repositories', 'represent', 'representation', 'representations', 'represented', 'representing', 'represents', 'reproduce', 'request', 'require', 'required', 'requires', 'requiring', 'reroute', 'rescue', 'research', 'researchers', 'reset', 'resides', 'residual', 'resize', 'resolve', 'resolving', 'resource', 'resources', 'respective', 'respectively', 'respond', 'response', 'responses', 'rest', 'restart', 'restore', 'restrict', 'result', 'resulted', 'resulting', 'results', 'rethinking', 'retry', 'return', 'returning', 'rev', 'revert', 'review', 'rhm', 'richard', 'richer', 'rid', 'ries', 'right', 'rigorous', 'ritter', 'rk', 'rkk', 'rkn', 'rm', 'rmdir', 'rn', 'rnm', 'rnn', 'rnnlm', 'rnns', 'robert', 'robust', 'rock', 'rojas', 'role', 'ronan', 'rong', 'root', 'root_entity', 'rotaru', 'rotaru2002', 'rounded', 'row', 'rows', 'rsync', 'rt', 'ruan', 'ruder', 'ruder12', 'rule', 'rules', 'run', 'running', 'runs', 'ruslan', 'ryan', 's2v', 'sadao', 'sadbuttrue', 'said', 'salakhutdinov', 'salzmann', 'sameer', 'sample', 'samuel', 'samy', 'sanghavi', 'sauerbraten', 'save', 'saw', 'say', 'saying', 'says', 'scalar', 'scalars', 'scale', 'scales', 'scan', 'schatzmann', 'schemes', 'schmidhuber', 'schmidhuber1997', 'schuller', 'schuster', 'science', 'score', 'scores', 'scp', 'scrape', 'scratch', 'screen', 'script', 'scripts', 'scroll', 'scrotwm', 'sda1', 'sdf', 'sdiff', 'search', 'sec', 'second', 'second_speaker', 'section', 'sed', 'seeing', 'seen', 'segmentation', 'select', 'selection', 'selective', 'self', 'selves', 'semantic', 'semantics', 'send', 'senior', 'sensors', 'sent', 'sentence', 'sentences', 'sentiment', 'seokhwan', 'separate', 'separating', 'separation', 'sepp', 'seq', 'seq2seq', 'sequence', 'sequences', 'sequential', 'serban', 'serdyuk', 'server', 'service', 'session', 'set', 'sets', 'setting', 'settings', 'setup', 'seven', 'sgaard', 'sgaard3', 'sgd', 'shall', 'shaoqing', 'share', 'shared', 'sharing', 'shawar', 'shed', 'shell', 'shes', 'shift', 'shopt', 'short', 'shortcomings', 'shorttext', 'shot', 'shouldn', 'shouldnt', 'showed', 'shown', 'showns', 'shows', 'shriberg', 'shrivastava', 'shutdown', 'si', 'sid1', 'sida', 'sigdial', 'sigmoid', 'sign', 'signal', 'signatures', 'significant', 'significantly', 'signup', 'silberman', 'silva', 'similar', 'similarity', 'similarly', 'simonsen', 'simple', 'simplicity', 'simplified', 'simply', 'simulate', 'simulation', 'simultaneously', 'single', 'sis', 'size', 'sizes', 'sjd', 'skip', 'skipconnections', 'sleep', 'slightly', 'slocate', 'slower', 'sluice', 'small', 'smaller', 'smartdimmer', 'smc', 'smiley', 'smoother', 'sms', 'smt', 'socher', 'social', 'soegaard', 'soft', 'softmax', 'software', 'solution', 'solving', 'somebody', 'somethan', 'somewhat', 'song', 'soon', 'sordoni', 'sorry', 'sort', 'sounds', 'source', 'sources', 'space', 'spaces', 'spam', 'sparse', 'sparsity', 'speaking', 'speaks', 'special', 'specific', 'specifically', 'specified', 'specify', 'specifying', 'speech', 'spirit', 'split', 'splits', 'spoken', 'spray', 'spreading', 'spring', 'springer', 'spurred', 'squared', 'sram', 'srl', 'ssh', 'ssid', 'stable', 'stacking', 'stand', 'standard', 'stands', 'start', 'started', 'starting', 'startup', 'stat', 'state', 'states', 'static', 'statistical', 'statistics', 'stefan', 'step', 'sterm', 'steven', 'stick', 'sticks', 'stitch', 'stochastic', 'stolcke', 'stop', 'stopping', 'stopwords', 'store', 'strace', 'stream', 'string', 'strong', 'strongest', 'strongly', 'structure', 'structured', 'structures', 'stuck', 'studies', 'studio', 'study', 'stuff', 'style', 'su', 'sub', 'submission', 'subsection', 'subsequent', 'subspace', 'subspaces', 'substance', 'substantial', 'substantially', 'subvectors', 'successful', 'successfully', 'sudo', 'sufficiently', 'suggest', 'suggestion', 'suggestions', 'suggests', 'suitable', 'sujay', 'sum', 'summarization', 'summary', 'sun', 'sungjin', 'sup', 'super', 'supervised', 'supervision', 'support', 'supports', 'sure', 'surface', 'survey', 'suspend', 'sutskever', 'svm', 'svms', 'swap', 'swbddamsl', 'swda', 'swirszcz', 'switchboard', 'symbol', 'symbolic', 'symbols', 'symlink', 'synaptic', 'sync', 'synergy', 'syntactic', 'synthesis', 'systems', 'systemsettings', 'table', 'tagged', 'tagger', 'tagging', 'tags', 'tagset', 'tail', 'taken', 'takes', 'taking', 'talamadupula', 'talking', 'tangent', 'tango', 'tanh', 'tar', 'target', 'task', 'tasks', 'tasksel', 'taylor', 'tbh', 'tc', 'tcpdump', 'team', 'technical', 'technique', 'techniques', 'technologies', 'technology', 'tee', 'telephone', 'tell', 'telling', 'temporal', 'temporarily', 'tends', 'tense', 'tenses', 'term', 'terminal', 'terms', 'tesauro', 'test', 'testbed', 'testing', 'tests', 'tetsuji', 'text', 'texts', 'textual', 'th', 'thank', 'thanks', 'thanx', 'thatll', 'thats', 'thatve', 'theano', 'theart', 'theirs', 'thenhthe', 'theodoros', 'theoretically', 'theory', 'thered', 'therell', 'thereof', 'therere', 'theres', 'thereto', 'theyd', 'theyll', 'theyre', 'theyve', 'thilo', 'thing', 'things', 'think', 'thinking', 'thomson', 'thou', 'thoughh', 'thousand', 'thread', 'throug', 'throw', 'throwing', 'throws', 'thx', 'til', 'time', 'timeout', 'times', 'timothy', 'tip', 'tired', 'tklinger', 'token', 'tokenize', 'tokenizer', 'tokens', 'tomas', 'tomorrow', 'took', 'toolkit', 'tools', 'topic', 'topics', 'total', 'touch', 'tour', 'tput', 'tr', 'trace', 'traceroute', 'track', 'tracking', 'trade', 'traffic', 'train', 'trainable', 'trained', 'training', 'transactions', 'transcribed', 'transcription', 'transfer', 'transform', 'transformation', 'translation', 'transmitted', 'travel', 'travels', 'tree', 'trees', 'trevor', 'tricks', 'tried', 'tries', 'trigeorgis', 'trouble', 'troubleshoot', 'truly', 'truncate', 'trung', 'truth', 'try', 'trying', 'ts', 'tsort', 'tsuruoka', 'tth', 'tty', 'tulip', 'turing', 'turn', 'turninternal', 'tweet', 'tweetnlp', 'tweets', 'twice', 'twitter', 'twitterdialoguecorpus', 'twitters', 'txt', 'type', 'types', 'typically', 'u0', 'u1', 'u2', 'uactionscript', 'uadd', 'uaiglx', 'ubuntu', 'ubuntu_13', 'ubuntu_5', 'ubuntudialoguecorpus', 'uc', 'ucfg', 'ucl', 'uclass', 'ucoded', 'ucommand', 'ucopyleft', 'uemulator', 'uextended', 'uf', 'ufilemaker', 'ufraw', 'ugtk', 'ui', 'uif2iso', 'uj', 'ujump', 'uk', 'ulimit', 'ulivecd', 'ultes', 'umask', 'umingw', 'umontreal', 'unable', 'unalias', 'uname', 'unavailable', 'unblock', 'uncomment', 'underlying', 'undersatnd', 'understand', 'understanding', 'unexpand', 'unfortunately', 'unified', 'unifies', 'uniformly', 'unigram', 'unii', 'uninstall', 'uniq', 'unique', 'unit', 'units', 'university', 'unknown', 'unless', 'unlike', 'unlikely', 'unmet', 'unplug', 'unrar', 'unset', 'unshar', 'unstructured', 'unsupervised', 'unto', 'uo', 'update', 'updated', 'updating', 'upgrade', 'upgraded', 'upgrading', 'upload', 'upper', 'uppercases', 'ups', 'ur', 'url', 'urls', 'urtasun', 'usa', 'usb', 'use', 'used', 'useful', 'usefully', 'usefulness', 'user', 'useradd', 'userdel', 'usermod', 'users', 'uses', 'ushareware', 'using', 'usually', 'utterance', 'utterances', 'uudecode', 'uuid', 'v2', 'vahabi', 'valid', 'validate', 'validation', 'value', 'valued', 'values', 'van', 'vandyke', 'var', 'variable', 'variables', 'variance', 'variant', 'variants', 'variation', 'variational', 'varied', 'various', 'vary', 'varying', 'vb', 've', 'vector', 'vectors', 'verb', 'verbs', 'verify', 'versa', 'versatility', 'version', 'vert', 'vga', 'vhred', 'vi', 'viable', 'vice', 'video', 'view', 'vim', 'vincent', 'vinyals', 'virtanen', 'virtualbox', 'virtualisation', 'vision', 'visit', 'viz', 'vj', 'vlad', 'vlc', 'vm', 'vmstat', 'vnc', 'vnc4server', 'vncviewer', 'vocabulary', 'vol', 'volume', 'vp', 'vries', 'vs', 'vuze', 'w0', 'w1', 'w2', 'wait', 'waleed', 'wang', 'wanna', 'want', 'wants', 'wasn', 'wasnt', 'watch', 'watson', 'way', 'wayne', 'ways', 'wb', 'wc', 'wealth', 'web', 'website', 'websites', 'wed', 'week', 'weight', 'weighted', 'weights', 'weinberger', 'weischedel', 'welcome', 'welcoming', 'wen', 'went', 'weren', 'werent', 'weston', 'weve', 'wf', 'whatll', 'whats', 'whereis', 'wheres', 'whim', 'whiptail', 'whoami', 'whod', 'wholl', 'whomever', 'whos', 'wi', 'wichert', 'widely', 'william', 'williams', 'willing', 'window', 'windows', 'wine', 'wireless', 'wise', 'wish', 'wj', 'wn', 'wn1', 'wo', 'wobbly', 'won', 'wondering', 'wont', 'word', 'word2vec', 'wordcloud', 'words', 'work', 'worked', 'working', 'works', 'workshop', 'world', 'worse', 'worth', 'wouldn', 'wouldnt', 'wow', 'write', 'writes', 'wsj', 'www', 'x02', 'x02h', 'x03', 'x05', 'x0c', 'x0c1', 'x0c10', 'x0c12', 'x0c3', 'x0cappendix', 'x0cchiyuan', 'x0ccorresponding', 'x0cd2', 'x0cdomains', 'x0cevaluators', 'x0cfigure', 'x0cfor', 'x0cin', 'x0clearning', 'x0cnamed', 'x0cpooling', 'x0creduces', 'x0creferences', 'x0cshai', 'x0cstop', 'x0ctable', 'x0ctask', 'x0cthe', 'x0ctwitter', 'x0cubuntu', 'x1', 'x14', 'x15', 'xargs', 'xchat', 'xchatgnome', 'xchm', 'xdg', 'xebia', 'xgl', 'xi', 'xiangyu', 'xiong', 'xipeng', 'xorg', 'xrdp', 'xsensors', 'xt', 'xuanjing', 'xubuntu', 'xz', 'y1', 'yang', 'ye', 'yeah', 'yes', 'yi', 'yid2', 'yield', 'yielded', 'yielding', 'yields', 'yij', 'yj', 'ym', 'yoav', 'yoon', 'yoshimasa', 'yoshua', 'youd', 'youll', 'young', 'youre', 'youve', 'z1', 'zaremba', 'zeiler', 'zeiler2012', 'zero', 'zhang', 'zhou', 'zhuang', 'zi', 'zip', 'zn', 'zn0', 'zn1', 'zoneminder', 'zoomed']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizor.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
    "    return only_ascii.decode()\n",
    "\n",
    "input_string = remove_accents(testdoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokens = sent_tokenize(input_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercase everything here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "b'Accepted as a conference paper at NAACL 2016\\n\\nSequential Short-Text Classification with\\nRecurrent and Convolutional Neural Networks\\n\\narXiv:1603.03827v1 [cs.CL] 12 Mar 2016\\n\\nJi Young Lee\\nMIT\\njjylee@mit.edu\\n\\nAbstract\\nRecent approaches based on artificial neural\\nnetworks (ANNs) have shown promising results for short-text classification. However,\\nmany short texts occur in sequences (e.g., sentences in a document or utterances in a dialog), and most existing ANN-based systems\\ndo not leverage the preceding short texts when\\nclassifying a subsequent one. In this work,\\nwe present a model based on recurrent neural\\nnetworks and convolutional neural networks\\nthat incorporates the preceding short texts.\\nOur model achieves state-of-the-art results on\\nthree different datasets for dialog act prediction.\\n\\n1\\n\\nIntroduction\\n\\nShort-text classification is an important task in\\nmany areas of natural language processing, including sentiment analysis, question answering, or dialog management. Many different approaches have\\nbeen developed for short-text classification, such\\nas using Support Vector Machines (SVMs) with\\nrule-based features (Silva et al., 2011), combining SVMs with naive Bayes (Wang and Manning,\\n2012), and building dependency trees with Conditional Random Fields (Nakagawa et al., 2010).\\nSeveral recent studies using ANNs have shown\\npromising results, including convolutional neural\\nnetworks (CNNs) (Kim, 2014; Blunsom et al., 2014;\\nKalchbrenner et al., 2014) and recursive neural networks (Socher et al., 2012).\\nMost ANN systems classify short texts in isolation, i.e., without considering preceding short texts.\\n\\n\\nThese authors contributed equally to this work.\\n\\nFranck Dernoncourt\\nMIT\\nfrancky@mit.edu\\n\\nHowever, short texts usually appear in sequence\\n(e.g., sentences in a document or utterances in a dialog), therefore using information from preceding\\nshort texts may improve the classification accuracy.\\nPrevious works on sequential short-text classification are mostly based on non-ANN approaches, such\\nas Hidden Markov Models (HMMs) (Reithinger and\\nKlesen, 1997), (Stolcke et al., 2000), maximum entropy (Ang et al., 2005), and naive Bayes (Lendvai\\nand Geertzen, 2007).\\nInspired by the performance of ANN-based systems for non-sequential short-text classification, we\\nintroduce a model based on recurrent neural networks (RNNs) and CNNs for sequential short-text\\nclassification, and evaluate it on the dialog act classification task. A dialog act characterizes an utterance\\nin a dialog based on a combination of pragmatic, semantic, and syntactic criteria. Its accurate detection\\nis useful for a range of applications, from speech\\nrecognition to automatic summarization (Stolcke et\\nal., 2000). Our model achieves state-of-the-art results on three different datasets.\\n\\n2\\n\\nModel\\n\\nOur model comprises two parts. The first part generates a vector representation for each short text using\\neither the RNN or CNN architecture, as discussed in\\nSection 2.1 and Figure 1. The second part classifies\\nthe current short text based on the vector representations of the current as well as a few preceding short\\ntexts, as presented in Section 2.2 and Figure 2.\\nWe denote scalars with italic lowercases (e.g.,\\nk, bf ), vectors with bold lowercases (e.g., s, xi ),\\nand matrices with italic uppercases (e.g., Wf ). We\\n\\n\\x0cPooling\\n\\nRNN\\n\\nRNN\\n\\nPooling\\n\\nRNN\\n\\nRNN\\n\\nConv\\n\\nConv\\n\\nConv\\n\\nConv\\n\\nFigure 1: RNN (left) and CNN (right) architectures for generating the vector representation s of a short text x1:` . For CNN, Conv\\nrefers to convolution operations, and the filter height h = 3 is used in this figure.\\n\\nFF2\\n\\nFF2\\n\\nFF1\\n\\nFF1\\n\\nFF1\\n\\nFF1\\n\\nFF1\\n\\nS2V\\n\\nS2V\\n\\nS2V\\n\\nS2V\\n\\nS2V\\n\\nS2V\\n\\nS2V\\n\\nFF2\\n\\nFF2\\n\\nS2V\\n\\nFF1\\n\\nFF1\\n\\nS2V\\n\\nS2V\\n\\nFigure 2: Four instances of the two-layer feedforward ANN used for predicting the probability distribution over the classes zi for\\nthe ith short-text Xi . S2V stands for short text to vector, which is the RNN/CNN architecture that generates si from Xi . From left\\nto right, the history sizes (d1 , d2 ) are (0, 0), (2, 0), (0, 2) and (1, 1). (0, 0) corresponds to the non-sequential classification case.\\n\\nuse the colon notation vi:j to denote the sequence of\\nvectors (vi , vi+1 , . . . , vj ).\\n2.1\\n\\nShort-text representation\\n\\nA given short text of length ` is represented as the sequence of m-dimensional word vectors x1:` , which\\nis used by the RNN or CNN model to produce the\\nn-dimensional short-text representation s.\\n2.1.1\\n\\nRNN-based short-text representation\\n\\nWe use a variant of RNN called Long Short Term\\nMemory (LSTM) (Hochreiter and Schmidhuber,\\n1997). For the tth word in the short-text, an LSTM\\ntakes as input xt , ht1 , ct1 and produces ht , ct\\nbased on the following formulas:\\nit = (Wi xt + Ui ht1 + bi )\\nft = (Wf xt + Uf ht1 + bf )\\nct = tanh(Wc xt + Uc ht1 + bc )\\nct = ft\\n\\nct1 + it\\n\\nct\\n\\not = (Wo xt + Uo ht1 + bo )\\nht = o t\\n\\ntanh(ct )\\n\\nwhere Wj  Rnm , Uj  Rnn are weight matrices and bj  Rn are bias vectors, for j  {i, f, c, o}.\\n\\nThe symbols () and tanh() refer to the elementwise sigmoid and hyperbolic tangent functions, and\\nis the element-wise multiplication. h0 = c0 = 0.\\nIn the pooling layer, the sequence of vectors h1:`\\noutput from the RNN layer are combined into a single vector s  Rn that represents the short-text, using one of the following mechanisms: last, mean,\\nand max pooling. Last pooling takes the last vector,\\ni.e., s =\\nPh` , mean pooling averages all vectors, i.e.,\\ns = 1` `t=1 ht , and max pooling takes the elementwise maximum of h1:` .\\n2.1.2\\n\\nCNN-based short-text representation\\n\\nUsing a filter Wf  Rhm of height h, a convolution operation on h consecutive word vectors starting from tth word outputs the scalar feature\\nct = ReLU(Wf  Xt:t+h1 + bf )\\nwhere Xt:t+h1  Rhm is the matrix whose ith\\nrow is xi  Rm , and bf  R is a bias. The symbol \\nrefers to the dot product and ReLU() is the elementwise rectified linear unit function.\\nWe perform convolution operations with n different filters, and denote the resulting features as\\nct  Rn , each of whose dimensions comes from a\\ndistinct filter. Repeating the convolution operations\\n\\n\\x0cfor each window of h consecutive words in the shorttext, we obtain c1:`h+1 . The short-text representation s  Rn is computed in the max pooling layer,\\nas the element-wise maximum of c1:`h+1 .\\n2.2\\n\\nFor MRDA, we use the train/validation/test splits\\nprovided with the datasets. For DSTC 4 and SwDA,\\nonly the train/test splits are provided.1 Table 1\\npresents statistics on the datasets.\\n\\nSequential short-text classification\\n\\nLet si be the n-dimensional short-text representation\\ngiven by the RNN or CNN architecture for the ith\\nshort text in the sequence. The sequence sid1 d2 : i\\nis fed into a two-layer feedforward ANN that predicts the class for the ith short text. The hyperparameters d1 , d2 are the history sizes used in the first\\nand second layers, respectively.\\nThe first layer takes as input sid1 d2 : i and outputs the sequence yid2 : i defined as\\n!\\nd1\\nX\\nyj = tanh\\nWd sjd + b1 , j  [i  d2 , i]\\nd=0\\n\\nwhere W0 , W1 , W2  Rkn are the weight matrices, b1  Rk is the bias vector, yj  Rk is the\\nclass representation, and k is the number of classes\\nfor the classification task.\\nSimilarly, the second layer takes as input the sequence of class representations yid2 :i and outputs\\nzi  Rk :\\n\\n\\nd2\\nX\\nzi = softmax \\nWj yij + b2 \\nj=0\\n\\nwhere U0 , U1 , U2  Rkk and b2  Rk are the\\nweight matrices and bias vector.\\nThe final output zi represents the probability distribution over the set of k classes for the ith shorttext: the j th element of zi corresponds to the probability that the ith short-text belongs to the j th class.\\n\\nDataset\\n\\n|C| |V |\\n\\nTrain\\n\\nValidation\\n\\nTest\\n\\nDSTC 4\\n\\n89\\n\\n6k\\n\\n24 (21k)\\n\\n5 (5k)\\n\\n6 (6k)\\n\\nMRDA\\n\\n5\\n\\n12k\\n\\n51 (78k)\\n\\n11 (16k)\\n\\n11 (15k)\\n\\nSwDA\\n\\n43\\n\\n20k 1003 (193k)\\n\\n112 (23k)\\n\\n19 (5k)\\n\\nTable 1: Dataset overview. |C| is the number of classes, |V |\\nthe vocabulary size. For the train, validation and test sets, we\\nindicate the number of dialogs (i.e., sequences) followed by the\\nnumber of utterances (i.e., short texts) in parenthesis.\\n\\n3.2\\n\\nTraining\\n\\nThe model is trained to minimize the negative loglikelihood of predicting the correct dialog acts of the\\nutterances in the train set, using stochastic gradient\\ndescent with the Adadelta update rule (Zeiler, 2012).\\nAt each gradient descent step, weight matrices, bias\\nvectors, and word vectors are updated. For regularization, dropout is applied after the pooling layer,\\nand early stopping is used on the validation set with\\na patience of 10 epochs.\\n\\n4\\n\\nResults and Discussion\\n\\nTo find effective hyperparameters, we varied one hyperparameter at a time while keeping the other ones\\nfixed. Table 2 presents our hyperparameter choices.\\nHyperparameter\\n\\nChoice\\n\\nExperiment Range\\n\\n100\\n\\n50  1000\\n\\nLSTM pooling\\n\\nmax\\n\\nmax, mean, last\\n\\nLSTM direction\\n\\nunidir.\\n\\nunidir., bidir.\\n\\n500\\n\\n50  1000\\n\\n3\\n\\n1  10\\n\\n0.5\\n\\n01\\n\\n200, 300\\n\\n25  300\\n\\nLSTM output dim. (n)\\n\\nCNN num. of filters (n)\\nCNN filter height (h)\\n\\n3\\n3.1\\n\\nDatasets and Experimental Setup\\nDatasets\\n\\nWe evaluate our model on the dialog act classification task using the following datasets:\\n DSTC 4: Dialog State Tracking Challenge 4 (Kim\\net al., 2015; Kim et al., 2016).\\n MRDA: ICSI Meeting Recorder Dialog Act Corpus (Janin et al., 2003; Shriberg et al., 2004). The\\n5 classes are introduced in (Ang et al., 2005).\\n SwDA: Switchboard Dialog Act Corpus (Jurafsky\\net al., 1997).\\n\\nDropout rate\\nWord vector dim. (m)\\n\\nTable 2: Experiments ranges and choices of hyperparameters.\\nUnidir refers to the regular RNNs presented in Section 2.1.1,\\nand bidir refers to bidirectional RNNs introduced in (Schuster\\nand Paliwal, 1997).\\n\\nWe initialized the word vectors with the 300dimensional word vectors pretrained with word2vec\\non Google News (Mikolov et al., 2013a; Mikolov\\net al., 2013b) for DSTC 4, and the 200-dimensional\\n1\\n\\nAll train/validation/test splits can be found at https://\\ngithub.com/Franck-Dernoncourt/naacl2016\\n\\n\\x0cd2\\n\\nDSTC4\\n\\nMRDA\\n\\nSwDA\\n\\nLSTM\\n\\nCNN\\n\\n0\\n\\n1\\n\\n2\\n\\n0\\n\\n1\\n\\n2\\n\\n0\\n\\n63.1 (62.4, 63.6)\\n\\n65.7 (65.6, 65.7)\\n\\n64.7 (63.9, 65.3)\\n\\n64.1 (63.5, 65.2)\\n\\n65.4 (64.7, 66.6)\\n\\n65.1 (63.2, 65.9)\\n\\n1\\n\\n65.8 (65.5, 66.1)\\n\\n65.7 (65.3, 66.1)\\n\\n64.8 (64.6, 65.1)\\n\\n65.3 (64.1, 65.9)\\n\\n65.1 (62.1, 66.2)\\n\\n64.9 (64.4, 65.6)\\n\\n2\\n\\n65.7 (65.0, 66.2)\\n\\n65.5 (64.4, 66.1)\\n\\n64.9 (64.6, 65.2)\\n\\n65.7 (64.9, 66.3)\\n\\n65.8 (65.2, 66.1)\\n\\n65.4 (64.5, 66.0)\\n\\n0\\n\\n82.8 (82.4, 83.1)\\n\\n83.2 (82.9, 83.4)\\n\\n82.9 (82.4, 83.4)\\n\\n83.2 (83.0, 83.4)\\n\\n83.5 (82.9, 84.0)\\n\\n83.8 (83.4, 84.2)\\n\\n1\\n\\n83.2 (82.6, 83.7)\\n\\n83.8 (83.5, 84.4)\\n\\n83.6 (83.2, 83.8)\\n\\n84.6 (84.5, 84.9)\\n\\n84.6 (84.4, 84.8)\\n\\n84.1 (83.8, 84.4)\\n\\n2\\n\\n84.1 (83.5, 84.4)\\n\\n83.9 (83.4, 84.7)\\n\\n83.3 (82.6, 84.2)\\n\\n84.4 (84.1, 84.8)\\n\\n84.6 (84.5, 84.7)\\n\\n84.4 (84.2, 84.7)\\n\\n0\\n\\n66.3 (65.1, 68.0)\\n\\n67.9 (66.3, 68.6)\\n\\n67.8 (66.7, 69.0)\\n\\n67.0 (65.3, 68.7)\\n\\n69.1 (68.5, 70.0)\\n\\n69.7 (69.2, 70.9)\\n\\n1\\n\\n68.4 (67.8, 68.8)\\n\\n67.8 (65.5, 68.9)\\n\\n67.3 (65.5, 69.5)\\n\\n69.9 (69.1, 70.9)\\n\\n69.8 (69.3, 70.6)\\n\\n69.9 (68.8, 70.6)\\n\\n2\\n\\n69.5 (68.9, 70.2)\\n\\n67.9 (66.5, 69.4)\\n\\n67.7 (66.9, 68.9)\\n\\n71.4 (70.4, 73.1)\\n\\n71.1 (70.2, 72.1)\\n\\n70.9 (69.7, 71.7)\\n\\nd1\\n\\nTable 3: Accuracy (%) on different architectures and history sizes d1 , d2 . For each setting, we report average (minimum, maximum)\\ncomputed on 5 runs. Sequential classification (d1 + d2 > 0) outperforms non-sequential classification (d1 = d2 = 0). Overall, the\\nCNN model outperformed the LSTM model for all datasets, albeit by a small margin except for SwDA. We also tried a variant of\\nthe LSTM model, gated recurrent units (Cho et al., 2014), but the results were generally lower than LSTM.\\n\\nword vectors pretrained with GloVe on Twitter (Pennington et al., 2014) for MRDA and\\nSwDA, as these choices yielded the best results\\namong all publicly available word2vec, GloVe,\\nSENNA (Collobert, 2011; Collobert et al., 2011)\\nand RNNLM (Mikolov et al., 2011) word vectors.\\nThe effects of the history sizes d1 and d2 for the\\nshort-text and the class representations, respectively,\\nare presented in Table 3 for both the LSTM and\\nCNN models. In both models, increasing d1 while\\nkeeping d2 = 0 improved the performances by 1.34.2 percentage points. Conversely, increasing d2\\nwhile keeping d1 = 0 yielded better results, but the\\nperformance increase was less pronounced: incorporating sequential information at the short-text representation level was more effective than at the class\\nrepresentation level.\\nUsing sequential information at both the shorttext representation level and the class representation level does not help in most cases and may even\\nlower the performances. We hypothesize that shorttext representations contain richer and more general information than class representations due to\\ntheir larger dimension. Class representations may\\nnot convey any additional information over shorttext representations, and are more likely to propagate errors from previous misclassifications.\\nTable 4 compares our results with the state-of-theart. Overall, our model shows competitive results,\\nwhile requiring no human-engineered features. Rigorous comparisons are challenging to draw, as many\\nimportant details such as text preprocessing and\\ntrain/valid/test split may vary, and many studies fail\\n\\nto perform several runs despite the randomness in\\nsome parts of the training process, such as weight\\ninitialization.\\nModel\\n\\nDSTC 4\\n\\nMRDA\\n\\nSwDA\\n\\nCNN\\n\\n65.5\\n\\n84.6\\n\\n73.1\\n\\nLSTM\\n\\n66.2\\n\\n84.3\\n\\n69.6\\n\\nMajority class\\n\\n25.8\\n\\n59.1\\n\\n33.7\\n\\nSVM\\n\\n57.0\\n\\n\\n\\n\\n\\nGraphical model\\n\\n\\n\\n81.3\\n\\n\\n\\nNaive Bayes\\n\\n\\n\\n82.0\\n\\n\\n\\nHMM\\n\\n\\n\\n\\n\\n71.0\\n\\nMemory-based Learning\\n\\n\\n\\n\\n\\n72.3\\n\\nInterlabeler agreement\\n\\n\\n\\n\\n\\n84.0\\n\\nTable 4: Accuracy (%) of our models and other methods from\\nthe literature. The majority class model predicts the most frequent class. SVM: (Dernoncourt et al., 2016). Graphical model:\\n(Ji and Bilmes, 2006). Naive Bayes: (Lendvai and Geertzen,\\n2007). HMM: (Stolcke et al., 2000). Memory-based Learning:\\n(Rotaru, 2002). All five models use features derived from transcribed words, as well as previous predicted dialog acts except\\nfor Naive Bayes. The interlabeler agreement could be obtained\\nonly for SwDA. For the CNN and LSTM models, the presented\\nresults are the test set accuracy of the run with the highest accuracy on the validation set.\\n\\n5\\n\\nConclusion\\n\\nIn this article we have presented an ANN-based approach to sequential short-text classification. We\\ndemonstrate that adding sequential information improves the quality of the predictions, and the performance depends on what sequential information is\\nused in the model. Our model achieves state-of-theart results on three different datasets for dialog act\\nprediction.\\n\\n\\x0cReferences\\n[Ang et al.2005] Jeremy Ang, Yang Liu, and Elizabeth\\nShriberg. 2005. Automatic dialog act segmentation\\nand classification in multiparty meetings. In ICASSP\\n(1), pages 10611064.\\n[Blunsom et al.2014] Phil Blunsom, Edward Grefenstette,\\nNal Kalchbrenner, et al. 2014. A convolutional neural network for modelling sentences. In Proceedings\\nof the 52nd Annual Meeting of the Association for\\nComputational Linguistics. Proceedings of the 52nd\\nAnnual Meeting of the Association for Computational\\nLinguistics.\\n[Cho et al.2014] Kyunghyun Cho, Bart van Merrienboer,\\nDzmitry Bahdanau, and Yoshua Bengio. 2014. On\\nthe properties of neural machine translation: Encoderdecoder approaches. arXiv preprint arXiv:1409.1259.\\n[Collobert et al.2011] Ronan Collobert, Jason Weston,\\nLeon Bottou, Michael Karlen, Koray Kavukcuoglu,\\nand Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine\\nLearning Research, 12:24932537.\\n[Collobert2011] Ronan Collobert. 2011. Deep learning for efficient discriminative parsing. In International Conference on Artificial Intelligence and Statistics, number EPFL-CONF-192374.\\n[Dernoncourt et al.2016] Franck Dernoncourt, Ji Young\\nLee, Trung H. Bui, and Hung H. Bui. 2016. AdobeMIT submission to the DSTC 4 Spoken Language Understanding pilot task. In 7th International Workshop\\non Spoken Dialogue Systems (IWSDS).\\n[Hochreiter and Schmidhuber1997] Sepp Hochreiter and\\nJurgen Schmidhuber. 1997. Long short-term memory.\\nNeural computation, 9(8):17351780.\\n[Janin et al.2003] Adam Janin, Don Baron, Jane Edwards,\\nDan Ellis, David Gelbart, Nelson Morgan, Barbara Peskin, Thilo Pfau, Elizabeth Shriberg, Andreas Stolcke,\\net al. 2003. The ICSI meeting corpus. In Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP03). 2003 IEEE International Conference on, volume 1, pages I364. IEEE.\\n[Ji and Bilmes2006] Gang Ji and Jeff Bilmes. 2006.\\nBackoff model training using partially observed data:\\napplication to dialog act tagging. In Proceedings of\\nthe main conference on Human Language Technology Conference of the North American Chapter of the\\nAssociation of Computational Linguistics, pages 280\\n287. Association for Computational Linguistics.\\n[Jurafsky et al.1997] Dan Jurafsky, Elizabeth Shriberg,\\nand Debra Biasca. 1997. Switchboard SWBDDAMSL\\nshallow-discourse-function\\nannotation\\ncoders manual.\\nInstitute of Cognitive Science\\nTechnical Report, pages 97102.\\n\\n[Kalchbrenner et al.2014] Nal Kalchbrenner, Edward\\nGrefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. arXiv\\npreprint arXiv:1404.2188.\\n[Kim et al.2015] Seokhwan Kim, Luis Fernando DHaro,\\nRafael E. Banchs, Jason Williams, and Matthew Henderson. 2015. Dialog State Tracking Challenge 4:\\nHandbook.\\n[Kim et al.2016] Seokhwan Kim, Luis Fernando DHaro,\\nRafael E. Banchs, Jason Williams, and Matthew Henderson. 2016. The Fourth Dialog State Tracking Challenge. In Proceedings of the 7th International Workshop on Spoken Dialogue Systems (IWSDS).\\n[Kim2014] Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of\\nthe 2014 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 17461751. Association\\nfor Computational Linguistics.\\n[Lendvai and Geertzen2007] Piroska Lendvai and Jeroen\\nGeertzen. 2007. Token-based chunking of turninternal dialogue act sequences. In Proceedings of the\\n8th SIGDIAL Workshop on Discourse and Dialogue,\\npages 174181.\\n[Mikolov et al.2011] Tomas Mikolov, Stefan Kombrink,\\nAnoop Deoras, Lukar Burget, and Jan Cernocky.\\n2011. Rnnlm-recurrent neural network language modeling toolkit. In Proc. of the 2011 ASRU Workshop,\\npages 196201.\\n[Mikolov et al.2013a] Tomas Mikolov, Kai Chen, Greg\\nCorrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv\\npreprint arXiv:1301.3781.\\n[Mikolov et al.2013b] Tomas Mikolov, Ilya Sutskever,\\nKai Chen, Greg S Corrado, and Jeff Dean. 2013b.\\nDistributed representations of words and phrases and\\ntheir compositionality. In Advances in neural information processing systems, pages 31113119.\\n[Nakagawa et al.2010] Tetsuji Nakagawa, Kentaro Inui,\\nand Sadao Kurohashi. 2010. Dependency tree-based\\nsentiment classification using CRFs with hidden variables. In Human Language Technologies: The 2010\\nAnnual Conference of the North American Chapter of\\nthe Association for Computational Linguistics, pages\\n786794. Association for Computational Linguistics.\\n[Pennington et al.2014] Jeffrey Pennington, Richard\\nSocher, and Christopher D Manning. 2014. GloVe:\\nglobal vectors for word representation. Proceedings\\nof the Empiricial Methods in Natural Language\\nProcessing (EMNLP 2014), 12:15321543.\\n[Reithinger and Klesen1997] Norbert Reithinger and\\nMartin Klesen. 1997. Dialogue act classification\\nusing language models. In EuroSpeech. Citeseer.\\n\\n\\x0c[Rotaru2002] Mihai Rotaru. 2002. Dialog act tagging using memory-based learning. Term project, University\\nof Pittsburgh, pages 255276.\\n[Schuster and Paliwal1997] Mike Schuster and Kuldip K\\nPaliwal. 1997. Bidirectional recurrent neural networks. Signal Processing, IEEE Transactions on,\\n45(11):26732681.\\n[Shriberg et al.2004] Elizabeth Shriberg, Raj Dhillon,\\nSonali Bhagat, Jeremy Ang, and Hannah Carvey.\\n2004. The ICSI meeting recorder dialog act (MRDA)\\ncorpus. Technical report, DTIC Document.\\n[Silva et al.2011] Joao Silva, Lusa Coheur, Ana Cristina\\nMendes, and Andreas Wichert. 2011. From symbolic\\nto sub-symbolic information in question classification.\\nArtificial Intelligence Review, 35(2):137154.\\n[Socher et al.2012] Richard Socher, Brody Huval,\\nChristopher D Manning, and Andrew Y Ng. 2012.\\nSemantic compositionality through recursive matrixvector spaces. In Proceedings of the 2012 Joint\\nConference on Empirical Methods in Natural\\nLanguage Processing and Computational Natural\\nLanguage Learning, pages 12011211. Association\\nfor Computational Linguistics.\\n[Stolcke et al.2000] Andreas Stolcke, Klaus Ries, Noah\\nCoccaro, Elizabeth Shriberg, Rebecca Bates, Daniel\\nJurafsky, Paul Taylor, Rachel Martin, Carol Van EssDykema, and Marie Meteer. 2000. Dialogue act\\nmodeling for automatic tagging and recognition of\\nconversational speech. Computational linguistics,\\n26(3):339373.\\n[Wang and Manning2012] Sida Wang and Christopher D\\nManning. 2012. Baselines and bigrams: Simple, good\\nsentiment and topic classification. In Proceedings of\\nthe 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages\\n9094. Association for Computational Linguistics.\\n[Zeiler2012] Matthew D Zeiler.\\n2012.\\nAdadelta:\\nAn adaptive learning rate method. arXiv preprint\\narXiv:1212.5701.\\n\\n\\x0c'\n"
     ]
    }
   ],
   "source": [
    "stopwords_ = list(stopwords.words('english'))\n",
    "stopwords_ = stopwords_ + (['\\x9d','\\n','\\xef','\\xac','\\x81','\\xe2','\\x80','\\x9c','\\xe2','\\x80','\\x9c','\\x0c','\\xef','\\x81','\\xc3','\\xbc','\\x0c','\\x88','\\x92','\\x82','\\x92','\\x99','\\x84','\\x93','\\x87','\\x97'])\n",
    "for word in stopwords_:\n",
    "    testdoc= testdoc.replace(str('\\n'), '')\n",
    "print(testdoc)\n",
    "charList = (list('1234567890'))\n",
    "charList.append(string.punctuation)\n",
    "for char in charList:\n",
    "    testdoc = testdoc.replace(str(word), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"b'Accepe   cfence pper  NAACL 2016\\\\n\\\\nSequenl Sh-Tex Clfcn wh\\\\nRecurn n Cvlunl Neurl Newk\\\\n\\\\nrXv:1603.03827v1 [c.CL] 12 Mr 2016\\\\n\\\\nJ Yung Lee\\\\nMIT\\\\njjlee@.eu\\\\n\\\\nAbrc\\\\nRecen pprc   rfcl neurl\\\\nnewk (ANN)  n prng ul f h-ex clfcn. H,\\\\nn h ex ccur n equence (e.g., enence n  cun  uernce n  lg), n  exng ANN- e\\\\n  lerge  pceng h ex wn\\\\nclfng  uquen e. In h wk,\\\\n pen  el   curn neurl\\\\nnewk n cvlunl neurl newk\\\\n ncpe  pceng h ex.\\\\nOur el c e---r ul \\\\n ffen e f lg c pcn.\\\\n\\\\n1\\\\n\\\\nInrucn\\\\n\\\\nSh-ex clfcn  n pn k n\\\\nn   nurl lnguge prceng, nclung enn nl, quen nrng,  lg ngen. Mn ffen pprc \\\\nen elpe f h-ex clfcn, \\\\n ung Sp Vec Mchne (SVM) wh\\\\nrule- feu (Slv e l., 2011), cbnng SVM wh n Be (Wng n Mnnng,\\\\n2012), n bulng epenenc e wh Cnl Rn Fel (Nkgw e l., 2010).\\\\nSerl cen ue ung ANN  n\\\\nprng ul, nclung cvlunl neurl\\\\nnewk (CNN) (K, 2014; Blun e l., 2014;\\\\nKlchbnner e l., 2014) n cur neurl newk (Scr e l., 2012).\\\\nM ANN e clf h ex n ln, .e., wh cerng pceng h ex.\\\\n\\\\n\\\\nTe uh c equ  h wk.\\\\n\\\\nFrnck Dernc\\\\nMIT\\\\nfrnck@.eu\\\\n\\\\nH, h ex uu pper n equence\\\\n(e.g., enence n  cun  uernce n  lg), fe ung nfn  pceng\\\\nh ex  pr  clfcn ccurc.\\\\nPvu wk  equenl h-ex clfcn  l   n-ANN pprc, \\\\n Hen Mrkv Mel (HMM) (Rehnger n\\\\nKleen, 1997), (Slcke e l., 2000), xu enrp (Ang e l., 2005), n n Be (Lenv\\\\nn Geerzen, 2007).\\\\nInp   perfnce  ANN- e f n-equenl h-ex clfcn, \\\\nnruce  el   curn neurl newk (RNN) n CNN f equenl h-ex\\\\nclfcn, n evlue    lg c clfcn k. A lg c chrcerze n uernce\\\\nn  lg    cbnn  prgc, enc, n ncc cr. I ccu eecn\\\\n ueful f  rnge  pplcn,  peech\\\\ncgnn  uc urzn (Slcke e\\\\nl., 2000). Our el c e---r ul   ffen e.\\\\n\\\\n2\\\\n\\\\nMel\\\\n\\\\nOur el cp w pr. T fr pr gene  c penn f ech h ex ung\\\\ner  RNN  CNN rccu,  cue n\\\\nSecn 2.1 n Fgu 1. T ec pr clfe\\\\n curn h ex    c penn   curn      pceng h\\\\nex,  pene n Secn 2.2 n Fgu 2.\\\\nWe ee clr wh lc lce (e.g.,\\\\nk, bf ), c wh bl lce (e.g., , x ),\\\\nn rce wh lc perce (e.g., Wf ). We\\\\n\\\\n\\\\x0cPlng\\\\n\\\\nRNN\\\\n\\\\nRNN\\\\n\\\\nPlng\\\\n\\\\nRNN\\\\n\\\\nRNN\\\\n\\\\nCv\\\\n\\\\nCv\\\\n\\\\nCv\\\\n\\\\nCv\\\\n\\\\nFgu 1: RNN (lef) n CNN (rgh) rccu f generng  c penn    h ex x1:` . F CNN, Cv\\\\nfer  cvlun pern, n  fler gh h = 3  ue n h fgu.\\\\n\\\\nFF2\\\\n\\\\nFF2\\\\n\\\\nFF1\\\\n\\\\nFF1\\\\n\\\\nFF1\\\\n\\\\nFF1\\\\n\\\\nFF1\\\\n\\\\nS2V\\\\n\\\\nS2V\\\\n\\\\nS2V\\\\n\\\\nS2V\\\\n\\\\nS2V\\\\n\\\\nS2V\\\\n\\\\nS2V\\\\n\\\\nFF2\\\\n\\\\nFF2\\\\n\\\\nS2V\\\\n\\\\nFF1\\\\n\\\\nFF1\\\\n\\\\nS2V\\\\n\\\\nS2V\\\\n\\\\nFgu 2: F nnce   w-ler feefwr ANN ue f pcng  prbbl r   cle z f\\\\n h h-ex X . S2V n f h ex  c, whch   RNN/CNN rccu  gene   X . Fr lef\\\\n rgh,  h ze (1 , 2 )  (0, 0), (2, 0), (0, 2) n (1, 1). (0, 0) cp   n-equenl clfcn ce.\\\\n\\\\nue  cl n v:j  ee  equence \\\\nc (v , v+1 , . . . , vj ).\\\\n2.1\\\\n\\\\nSh-ex penn\\\\n\\\\nA gn h ex  lengh `  pene   equence  -nl w c x1:` , whch\\\\n ue   RNN  CNN el  pruce \\\\nn-nl h-ex penn .\\\\n2.1.1\\\\n\\\\nRNN- h-ex penn\\\\n\\\\nWe ue  vrn  RNN ce Lg Sh Ter\\\\nMe (LSTM) (Hcr n Schhur,\\\\n1997). F  h w n  h-ex, n LSTM\\\\nke  npu x , h1 , c1 n pruce h , c\\\\n   fg ful:\\\\n = (W x + U h1 + b )\\\\nf = (Wf x + Uf h1 + bf )\\\\nc = nh(Wc x + Uc h1 + bc )\\\\nc = f\\\\n\\\\nc1 + \\\\n\\\\nc\\\\n\\\\ = (W x + U h1 + b )\\\\nh =  \\\\n\\\\nnh(c )\\\\n\\\\nw Wj  Rn , Uj  Rnn  gh rce n bj  Rn  b c, f j  {, f, c, }.\\\\n\\\\nT bl () n nh() fer   elen g n hperblc ngen funcn, n\\\\n  elen- ulplcn. h0 = c0 = 0.\\\\nIn  plng ler,  equence  c h1:`\\\\npu   RNN ler  cbne n  ngle c   Rn  pen  h-ex, ung e   fg chn: l, n,\\\\nn x plng. L plng ke  l c,\\\\n.e.,  =\\\\nPh` , n plng rge  c, .e.,\\\\n = 1` `=1 h , n x plng ke  elen xu  h1:` .\\\\n2.1.2\\\\n\\\\nCNN- h-ex penn\\\\n\\\\nUng  fler Wf  Rh  gh h,  cvlun pern  h cecu w c rng  h w pu  clr feu\\\\nc = ReLU(Wf  X:+h1 + bf )\\\\nw X:+h1  Rh   rx e h\\\\nrw  x  R , n bf  R   b. T bl \\\\nfer    pruc n ReLU()   elen cfe lner un funcn.\\\\nWe perf cvlun pern wh n ffen fler, n ee  ulng feu \\\\nc  Rn , ech  e n c  \\\\nnc fler. Repeng  cvlun pern\\\\n\\\\n\\\\x0cf ech wnw  h cecu w n  x,  bn c1:`h+1 . T h-ex penn   Rn  cpue n  x plng ler,\\\\n  elen- xu  c1:`h+1 .\\\\n2.2\\\\n\\\\nF MRDA,  ue  rn/vln/e pl\\\\npr wh  e. F DSTC 4 n SwDA,\\\\nl  rn/e pl  pr.1 Tble 1\\\\npen c   e.\\\\n\\\\nSequenl h-ex clfcn\\\\n\\\\nLe    n-nl h-ex penn\\\\ngn   RNN  CNN rccu f  h\\\\nh ex n  equence. T equence 1 2 : \\\\n fe n  w-ler feefwr ANN  pc  cl f  h h ex. T hperpr 1 , 2   h ze ue n  fr\\\\nn ec ler, pecl.\\\\nT fr ler ke  npu 1 2 :  n pu  equence 2 :  efne \\\\n!\\\\n1\\\\nX\\\\nj = nh\\\\nW j + b1 , j  [  2 , ]\\\\n=0\\\\n\\\\nw W0 , W1 , W2  Rkn   gh rce, b1  Rk   b c, j  Rk  \\\\ncl penn, n k   nur  cle\\\\nf  clfcn k.\\\\nSlrl,  ec ler ke  npu  equence  cl penn 2 : n pu\\\\nz  Rk :\\\\n\\\\n\\\\n2\\\\nX\\\\nz = x \\\\nWj j + b2 \\\\nj=0\\\\n\\\\nw U0 , U1 , U2  Rkk n b2  Rk  \\\\ngh rce n b c.\\\\nT fnl pu z pen  prbbl r   e  k cle f  h x:  j h elen  z cp   prbbl   h h-ex lg   j h cl.\\\\n\\\\nDe\\\\n\\\\n|C| |V |\\\\n\\\\nTrn\\\\n\\\\nVln\\\\n\\\\nTe\\\\n\\\\nDSTC 4\\\\n\\\\n89\\\\n\\\\n6k\\\\n\\\\n24 (21k)\\\\n\\\\n5 (5k)\\\\n\\\\n6 (6k)\\\\n\\\\nMRDA\\\\n\\\\n5\\\\n\\\\n12k\\\\n\\\\n51 (78k)\\\\n\\\\n11 (16k)\\\\n\\\\n11 (15k)\\\\n\\\\nSwDA\\\\n\\\\n43\\\\n\\\\n20k 1003 (193k)\\\\n\\\\n112 (23k)\\\\n\\\\n19 (5k)\\\\n\\\\nTble 1: De w. |C|   nur  cle, |V |\\\\n vcbulr ze. F  rn, vln n e e, \\\\nnce  nur  lg (.e., equence) f  \\\\nnur  uernce (.e., h ex) n pn.\\\\n\\\\n3.2\\\\n\\\\nTrnng\\\\n\\\\nT el  rne  nze  neg lglkelh  pcng  cc lg c  \\\\nuernce n  rn e, ung cc gn\\\\necen wh  Ael e rule (Zeler, 2012).\\\\nA ech gn ecen ep, gh rce, b\\\\nc, n w c  e. F gulrzn, rp  pple fer  plng ler,\\\\nn erl ppng  ue   vln e wh\\\\n pence  10 epch.\\\\n\\\\n4\\\\n\\\\nReul n Dcu\\\\n\\\\nT fn effec hperpr,  v e hperpr    whle keepng  r e\\\\nfxe. Tble 2 pen  hperpr chce.\\\\nHperpr\\\\n\\\\nChce\\\\n\\\\nExpern Rnge\\\\n\\\\n100\\\\n\\\\n50  1000\\\\n\\\\nLSTM plng\\\\n\\\\nx\\\\n\\\\nx, n, l\\\\n\\\\nLSTM cn\\\\n\\\\nunr.\\\\n\\\\nunr., br.\\\\n\\\\n500\\\\n\\\\n50  1000\\\\n\\\\n3\\\\n\\\\n1  10\\\\n\\\\n0.5\\\\n\\\\n01\\\\n\\\\n200, 300\\\\n\\\\n25  300\\\\n\\\\nLSTM pu . (n)\\\\n\\\\nCNN nu.  fler (n)\\\\nCNN fler gh (h)\\\\n\\\\n3\\\\n3.1\\\\n\\\\nDe n Expernl Se\\\\nDe\\\\n\\\\nWe evlue  el   lg c clfcn k ung  fg e:\\\\n DSTC 4: Dlg Se Trckng Cnge 4 (K\\\\ne l., 2015; K e l., 2016).\\\\n MRDA: ICSI Meeng Recer Dlg Ac Cpu (Jnn e l., 2003; Shrrg e l., 2004). T\\\\n5 cle  nruce n (Ang e l., 2005).\\\\n SwDA: Swchb Dlg Ac Cpu (Jurfk\\\\ne l., 1997).\\\\n\\\\nDrp \\\\nW c . ()\\\\n\\\\nTble 2: Expern rnge n chce  hperpr.\\\\nUnr fer   gulr RNN pene n Secn 2.1.1,\\\\nn br fer  bcnl RNN nruce n (Schuer\\\\nn Plwl, 1997).\\\\n\\\\nWe nlze  w c wh  300nl w c prne wh w2c\\\\n Ggle New (Mklv e l., 2013; Mklv\\\\ne l., 2013b) f DSTC 4, n  200-nl\\\\n1\\\\n\\\\nA rn/vln/e pl cn  fun  hp://\\\\nghub.c/Frnck-Dernc/ncl2016\\\\n\\\\n\\\\x0c2\\\\n\\\\nDSTC4\\\\n\\\\nMRDA\\\\n\\\\nSwDA\\\\n\\\\nLSTM\\\\n\\\\nCNN\\\\n\\\\n0\\\\n\\\\n1\\\\n\\\\n2\\\\n\\\\n0\\\\n\\\\n1\\\\n\\\\n2\\\\n\\\\n0\\\\n\\\\n63.1 (62.4, 63.6)\\\\n\\\\n65.7 (65.6, 65.7)\\\\n\\\\n64.7 (63.9, 65.3)\\\\n\\\\n64.1 (63.5, 65.2)\\\\n\\\\n65.4 (64.7, 66.6)\\\\n\\\\n65.1 (63.2, 65.9)\\\\n\\\\n1\\\\n\\\\n65.8 (65.5, 66.1)\\\\n\\\\n65.7 (65.3, 66.1)\\\\n\\\\n64.8 (64.6, 65.1)\\\\n\\\\n65.3 (64.1, 65.9)\\\\n\\\\n65.1 (62.1, 66.2)\\\\n\\\\n64.9 (64.4, 65.6)\\\\n\\\\n2\\\\n\\\\n65.7 (65.0, 66.2)\\\\n\\\\n65.5 (64.4, 66.1)\\\\n\\\\n64.9 (64.6, 65.2)\\\\n\\\\n65.7 (64.9, 66.3)\\\\n\\\\n65.8 (65.2, 66.1)\\\\n\\\\n65.4 (64.5, 66.0)\\\\n\\\\n0\\\\n\\\\n82.8 (82.4, 83.1)\\\\n\\\\n83.2 (82.9, 83.4)\\\\n\\\\n82.9 (82.4, 83.4)\\\\n\\\\n83.2 (83.0, 83.4)\\\\n\\\\n83.5 (82.9, 84.0)\\\\n\\\\n83.8 (83.4, 84.2)\\\\n\\\\n1\\\\n\\\\n83.2 (82.6, 83.7)\\\\n\\\\n83.8 (83.5, 84.4)\\\\n\\\\n83.6 (83.2, 83.8)\\\\n\\\\n84.6 (84.5, 84.9)\\\\n\\\\n84.6 (84.4, 84.8)\\\\n\\\\n84.1 (83.8, 84.4)\\\\n\\\\n2\\\\n\\\\n84.1 (83.5, 84.4)\\\\n\\\\n83.9 (83.4, 84.7)\\\\n\\\\n83.3 (82.6, 84.2)\\\\n\\\\n84.4 (84.1, 84.8)\\\\n\\\\n84.6 (84.5, 84.7)\\\\n\\\\n84.4 (84.2, 84.7)\\\\n\\\\n0\\\\n\\\\n66.3 (65.1, 68.0)\\\\n\\\\n67.9 (66.3, 68.6)\\\\n\\\\n67.8 (66.7, 69.0)\\\\n\\\\n67.0 (65.3, 68.7)\\\\n\\\\n69.1 (68.5, 70.0)\\\\n\\\\n69.7 (69.2, 70.9)\\\\n\\\\n1\\\\n\\\\n68.4 (67.8, 68.8)\\\\n\\\\n67.8 (65.5, 68.9)\\\\n\\\\n67.3 (65.5, 69.5)\\\\n\\\\n69.9 (69.1, 70.9)\\\\n\\\\n69.8 (69.3, 70.6)\\\\n\\\\n69.9 (68.8, 70.6)\\\\n\\\\n2\\\\n\\\\n69.5 (68.9, 70.2)\\\\n\\\\n67.9 (66.5, 69.4)\\\\n\\\\n67.7 (66.9, 68.9)\\\\n\\\\n71.4 (70.4, 73.1)\\\\n\\\\n71.1 (70.2, 72.1)\\\\n\\\\n70.9 (69.7, 71.7)\\\\n\\\\n1\\\\n\\\\nTble 3: Accurc (%)  ffen rccu n h ze 1 , 2 . F ech eng,  p rge (nu, xu)\\\\ncpue  5 run. Sequenl clfcn (1 + 2 > 0) perf n-equenl clfcn (1 = 2 = 0). Or, \\\\nCNN el perf  LSTM el f  e, l    rgn excep f SwDA. We l   vrn \\\\n LSTM el, ge curn un (Ch e l., 2014),   ul  gener l hn LSTM.\\\\n\\\\nw c prne wh GlVe  Tr (Pennngn e l., 2014) f MRDA n\\\\nSwDA,  e chce ele   ul\\\\ng  publcl vlble w2c, GlVe,\\\\nSENNA (C, 2011; C e l., 2011)\\\\nn RNNLM (Mklv e l., 2011) w c.\\\\nT effec   h ze 1 n 2 f \\\\nh-ex n  cl penn, pecl,\\\\n pene n Tble 3 f   LSTM n\\\\nCNN el. In  el, ncng 1 whle\\\\nkeepng 2 = 0 pr  perfnce  1.34.2 percenge p. Cl, ncng 2\\\\nwhle keepng 1 = 0 ele er ul,  \\\\nperfnce nce  le prunce: ncpng equenl nfn   h-ex penn lel  e effec hn   cl\\\\npenn lel.\\\\nUng equenl nfn    x penn lel n  cl penn lel e  lp n  ce n  en\\\\nl  perfnce. We hpze  x penn cn rcr n e generl nfn hn cl penn ue \\\\nr lrger n. Cl penn \\\\n c n nl nfn  x penn, n  e lkel  prpge err  pvu clfcn.\\\\nTble 4 cp  ul wh  e--r. Or,  el  cpe ul,\\\\nwhle qurng  hun-engnee feu. Rgu cpr  cngng  rw,  n\\\\npn el   ex pprceng n\\\\nrn/vl/e pl  vr, n n ue fl\\\\n\\\\n perf erl run epe  rnne n\\\\n pr   rnng prce,   gh\\\\nnlzn.\\\\nMel\\\\n\\\\nDSTC 4\\\\n\\\\nMRDA\\\\n\\\\nSwDA\\\\n\\\\nCNN\\\\n\\\\n65.5\\\\n\\\\n84.6\\\\n\\\\n73.1\\\\n\\\\nLSTM\\\\n\\\\n66.2\\\\n\\\\n84.3\\\\n\\\\n69.6\\\\n\\\\nMj cl\\\\n\\\\n25.8\\\\n\\\\n59.1\\\\n\\\\n33.7\\\\n\\\\nSVM\\\\n\\\\n57.0\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nGrphcl el\\\\n\\\\n\\\\n\\\\n81.3\\\\n\\\\n\\\\n\\\\nN Be\\\\n\\\\n\\\\n\\\\n82.0\\\\n\\\\n\\\\n\\\\nHMM\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n71.0\\\\n\\\\nMe- Lernng\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n72.3\\\\n\\\\nIner gen\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n84.0\\\\n\\\\nTble 4: Accurc (%)   el n r h \\\\n leru. T j cl el pc   fquen cl. SVM: (Dernc e l., 2016). Grphcl el:\\\\n(J n Bl, 2006). N Be: (Lenv n Geerzen,\\\\n2007). HMM: (Slcke e l., 2000). Me- Lernng:\\\\n(Rru, 2002). A f el ue feu er  rncr w,    pvu pce lg c excep\\\\nf N Be. T ner gen cul  bne\\\\nl f SwDA. F  CNN n LSTM el,  pene\\\\nul   e e ccurc   run wh  hg ccurc   vln e.\\\\n\\\\n5\\\\n\\\\nCclu\\\\n\\\\nIn h rcle   pene n ANN- pprch  equenl h-ex clfcn. We\\\\ne  ng equenl nfn pr  qul   pcn, n  perfnce epen   equenl nfn \\\\nue n  el. Our el c e--r ul   ffen e f lg c\\\\npcn.\\\\n\\\\n\\\\x0cRefence\\\\n[Ang e l.2005] Je Ang, Yng Lu, n Elzh\\\\nShrrg. 2005. Auc lg c egnn\\\\nn clfcn n ulpr eng. In ICASSP\\\\n(1), pge 10611064.\\\\n[Blun e l.2014] Phl Blun, Ewr Gfenee,\\\\nNl Klchbnner, e l. 2014. A cvlunl neurl newk f eng enence. In Prceeng\\\\n  52n Annul Meeng   Acn f\\\\nCpunl Lnguc. Prceeng   52n\\\\nAnnul Meeng   Acn f Cpunl\\\\nLnguc.\\\\n[Ch e l.2014] Kunghun Ch, Br vn Mernr,\\\\nDzr Bhnu, n Yhu Beng. 2014. On\\\\n prpe  neurl chne rnln: Encecer pprc. rXv pprn rXv:1409.1259.\\\\n[C e l.2011] Rn C, J Wen,\\\\nLe Bu, Mcl Krlen, K Kvukcuglu,\\\\nn Pl Kuk. 2011. Nurl lnguge prceng (l)  crch. T Jnl  Mchne\\\\nLernng Reerch, 12:24932537.\\\\n[C2011] Rn C. 2011. Deep lernng f effcen crn prng. In Inernnl Cfence  Arfcl Inegence n Sc, nur EPFL-CONF-192374.\\\\n[Dernc e l.2016] Frnck Dernc, J Yung\\\\nLee, Trung H. Bu, n Hung H. Bu. 2016. AMIT ub   DSTC 4 Spken Lnguge Unernng pl k. In 7h Inernnl Wkhp\\\\n Spken Dlgue Se (IWSDS).\\\\n[Hcr n Schhur1997] Sepp Hcr n\\\\nJurgen Schhur. 1997. Lg h-er .\\\\nNeurl cpun, 9(8):17351780.\\\\n[Jnn e l.2003] A Jnn, D Br, Jne Ewr,\\\\nDn E, Dv Gelbr, Nel Mgn, Brbr Pekn, Thl Pfu, Elzh Shrrg, An Slcke,\\\\ne l. 2003. T ICSI eng cpu. In Acuc, Speech, n Sgnl Prceng, 2003. Prceeng.(ICASSP03). 2003 IEEE Inernnl Cfence , vlu 1, pge I364. IEEE.\\\\n[J n Bl2006] Gng J n Jeff Bl. 2006.\\\\nBckf el rnng ung pr r :\\\\npplcn  lg c ggng. In Prceeng \\\\n n cfence  Hun Lnguge Techlg Cfence   Nh Arcn Chper  \\\\nAcn  Cpunl Lnguc, pge 280\\\\n287. Acn f Cpunl Lnguc.\\\\n[Jurfk e l.1997] Dn Jurfk, Elzh Shrrg,\\\\nn Debr Bc. 1997. Swchb SWBDDAMSL\\\\nhw-ce-funcn\\\\nnn\\\\ncer nul.\\\\nInue  Cgn Scence\\\\nTechncl Rep, pge 97102.\\\\n\\\\n[Klchbnner e l.2014] Nl Klchbnner, Ewr\\\\nGfenee, n Phl Blun. 2014. A cvlunl neurl newk f eng enence. rXv\\\\npprn rXv:1404.2188.\\\\n[K e l.2015] Sekhwn K, Lu Fernn DHr,\\\\nRfel E. Bnch, J W, n Mw Hener. 2015. Dlg Se Trckng Cnge 4:\\\\nHnbk.\\\\n[K e l.2016] Sekhwn K, Lu Fernn DHr,\\\\nRfel E. Bnch, J W, n Mw Hener. 2016. T Fh Dlg Se Trckng Cnge. In Prceeng   7h Inernnl Wkhp  Spken Dlgue Se (IWSDS).\\\\n[K2014] Y K. 2014. Cvlunl neurl newk f enence clfcn. In Prceeng \\\\n 2014 Cfence  Eprcl Meh n Nurl\\\\nLnguge Prceng, pge 17461751. Acn\\\\nf Cpunl Lnguc.\\\\n[Lenv n Geerzen2007] Prk Lenv n Jen\\\\nGeerzen. 2007. Tken- chunkng  urnnernl lgue c equence. In Prceeng  \\\\n8h SIGDIAL Wkhp  Dce n Dlgue,\\\\npge 174181.\\\\n[Mklv e l.2011] T Mklv, Sefn Kbrnk,\\\\nAp De, Lukr Burge, n Jn Cerck.\\\\n2011. Rnnl-curn neurl newk lnguge elng lk. In Prc.   2011 ASRU Wkhp,\\\\npge 196201.\\\\n[Mklv e l.2013] T Mklv, K Cn, Gg\\\\nCr, n Jeff Den. 2013. Effcen en  w penn n c pce. rXv\\\\npprn rXv:1301.3781.\\\\n[Mklv e l.2013b] T Mklv, Il Suker,\\\\nK Cn, Gg S Cr, n Jeff Den. 2013b.\\\\nD penn  w n ph n\\\\nr cpnl. In Avnce n neurl nfn prceng e, pge 31113119.\\\\n[Nkgw e l.2010] Teuj Nkgw, Kenr Inu,\\\\nn S Kurh. 2010. Depenenc e-\\\\nenn clfcn ung CRF wh n vrble. In Hun Lnguge Techlge: T 2010\\\\nAnnul Cfence   Nh Arcn Chper \\\\n Acn f Cpunl Lnguc, pge\\\\n786794. Acn f Cpunl Lnguc.\\\\n[Pennngn e l.2014] Jeff Pennngn, Rchr\\\\nScr, n Chrpr D Mnnng. 2014. GlVe:\\\\nglbl c f w penn. Prceeng\\\\n  Eprcl Meh n Nurl Lnguge\\\\nPrceng (EMNLP 2014), 12:15321543.\\\\n[Rehnger n Kleen1997] Nr Rehnger n\\\\nMrn Kleen. 1997. Dlgue c clfcn\\\\nung lnguge el. In EurSpeech. Ceeer.\\\\n\\\\n\\\\x0c[Rru2002] Mh Rru. 2002. Dlg c ggng ung - lernng. Ter prjec, Unr\\\\n Pburgh, pge 255276.\\\\n[Schuer n Plwl1997] Mke Schuer n Kulp K\\\\nPlwl. 1997. Bcnl curn neurl newk. Sgnl Prceng, IEEE Trncn ,\\\\n45(11):26732681.\\\\n[Shrrg e l.2004] Elzh Shrrg, Rj Dh,\\\\nSl Bhg, Je Ang, n Hnnh Cr.\\\\n2004. T ICSI eng cer lg c (MRDA)\\\\ncpu. Techncl p, DTIC Dcun.\\\\n[Slv e l.2011] J Slv, Lu Cur, An Crn\\\\nMene, n An Wcr. 2011. Fr blc\\\\n ub-blc nfn n quen clfcn.\\\\nArfcl Inegence Rew, 35(2):137154.\\\\n[Scr e l.2012] Rchr Scr, Br Huvl,\\\\nChrpr D Mnnng, n Anw Y Ng. 2012.\\\\nSenc cpnl  cur rxc pce. In Prceeng   2012 J\\\\nCfence  Eprcl Meh n Nurl\\\\nLnguge Prceng n Cpunl Nurl\\\\nLnguge Lernng, pge 12011211. Acn\\\\nf Cpunl Lnguc.\\\\n[Slcke e l.2000] An Slcke, Klu Re, Nh\\\\nCccr, Elzh Shrrg, Recc Be, Dnel\\\\nJurfk, Pul Tl, Rcl Mrn, Crl Vn EDke, n M Meeer. 2000. Dlgue c\\\\nelng f uc ggng n cgnn \\\\ncrnl peech. Cpunl lnguc,\\\\n26(3):339373.\\\\n[Wng n Mnnng2012] S Wng n Chrpr D\\\\nMnnng. 2012. Belne n bgr: Sple, g\\\\nenn n pc clfcn. In Prceeng \\\\n 50h Annul Meeng   Acn f Cpunl Lnguc: Sh Pper-Vlu 2, pge\\\\n9094. Acn f Cpunl Lnguc.\\\\n[Zeler2012] Mw D Zeler.\\\\n2012.\\\\nAel:\\\\nAn p lernng  h. rXv pprn\\\\nrXv:1212.5701.\\\\n\\\\n\\\\x0c'\""
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "testdoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [sent for sent in map(word_tokenize, sent_tokens)]\n",
    "list(enumerate(tokens))\n",
    "tokens_lower = [[word.lower() for word in sent]\n",
    "                 for sent in tokens]\n",
    "tokens_filtered = list(map(filter_tokens, tokens_lower))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "test = pd.read_csv('data/full_texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2415"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'1711.07441v1'"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "list(test['id'])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'1109.2388v1'"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "list(test['id'])[-199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7968"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "list(data['id']).index('1109.2388v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "12095"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "list(data['id']).index('1711.07441v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}